<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[这里是目录]]></title>
    <url>%2F2048%2F03%2F18%2Fhello%2F</url>
    <content type="text"><![CDATA[Kaggle 数据挖掘 人脸识别相关 深度学习与神经网络-吴恩达 机器学习实战系列 机器学习相关库 ### 基础 hexo建站相关 python基础]]></content>
  </entry>
  <entry>
    <title><![CDATA[Kaggle比赛入门-房价预测]]></title>
    <url>%2F2018%2F03%2F21%2FKaggle%2FKaggle%E6%AF%94%E8%B5%9B%E5%85%A5%E9%97%A8-%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[步骤 获取数据 探究数据（可视化+清洗） 设计并转换特征和目标变量 建立一个模型 制作并提交预测 获取数据 探究数据 查看目标函数与理解相关业务 123456plt.figure()plt.subplot(1,2,1)plt.hist(train.SalePrice, color='blue')plt.subplot(1,2,2)plt.hist(target, color='blue')plt.show() # 展示 数据分类-&gt; 数值特征和类别特征 方法一 1select_dtypes(include=[np.number]) 方法二 123features = pd.concat([train, test],keys=['train','test'])numeric_feats = features.dtypes[features.dtypes!="object"].indexcategorical_feats = features.dtypes[features.dtypes=="object"].index 查看特征与目标变量的关系 数值特征 通过seaborn的regplot函数作箱形图来显示类别特征与目标变量之间的关系 123456789def jointplot(x,y,**kwargs): try: sns.regplot(x=x,y=y) except Exception: print(x.value_counts()) numeric_feats = numeric_feats.drop("SalePrice")f = pd.melt(train, id_vars=['SalePrice'],value_vars=numeric_feats)g = sns.FacetGrid(f,col='variable',col_wrap=3,sharex=False,sharey=False,size=5)g = g.map(jointplot,"value","SalePrice") 类别特征 通过seaborn的boxplot()函数作箱形图来显示类别特征与目标变量之间的关系 12345678910for c in categorical_feats: train[c] = train[c].astype('category') if train[c].isnull().any(): train[c] = train[c].cat.add_categories(["Missing"]) train[c] = train[c].fillna("Missing")def boxplot(x,y,**kwargs): sns.boxplot(x=x,y=y)f = pd.melt(train,id_vars=['SalePrice'],value_vars=categorical_feats)g = sns.FacetGrid(f,col='variable',col_wrap=3,sharex=False,sharey=False,size=5)g = g.map(boxplot,"value","SalePrice") 整体关系 通过DataFrame.corr()方法显示列之间的相关性（或关系），可以用来研究特征与目标变量的亲密程度 12numeric_features = train.select_dtypes(include=[np.number]) corr = numeric_features.corr() 通过seaborn的heatmap()函数作热力图显示 123plt.subplots(figsize=(12,10))corrmat = train.corr()g = sns.heatmap(train.corr()) 缺失值情况 通过isnull().sum() 源码 参考资料 Getting Started with Kaggle: House Prices Competition Kaggle实践 -- 房价预测top15%方案 scipy.stats.boxcox1p]]></content>
      <categories>
        <category>Kaggle</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow+opencv2实现人脸识别]]></title>
    <url>%2F2018%2F03%2F13%2F%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E5%8F%8A%E8%AF%86%E5%88%ABpython%E5%AE%9E%E7%8E%B0%2F%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E5%8F%8A%E8%AF%86%E5%88%ABpython%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[配置 python3 Tensorflow opencv2 步骤 获取并显示摄像头实时视频&amp;读取本地视频 从实时视频流中识别出人脸区域 准备人脸数据 建立CNN（卷积神经网络）模型 利用keras库训练人脸识别模型 利用训练数据进行识别人脸 获取并显示摄像头实时视频&amp;读取本地视频 利用OpenCV读取视频流 12345678cv2.namedWindow(window_name)# 视频来源cap = cv2.VideoCapture(camera_idx) # 读取视频信息while cap.isOpened(): frame = cap.read() #读取一帧数据 # 显示图像 cv2.imshow(window_name, frame) 释放视频信息并销毁所有窗口 123# 释放摄像头并销毁所有窗口cap.release()cv2.destroyAllWindows() 从实时视频流中识别出人脸区域 使用人脸识别分类器 opencv自带许多个人脸识别分类器，可以看下官方源码 123456# 使用人脸识别分类器classfier = cv2.CascadeClassifier("haarcascade_frontalface_alt2.xml")# 识别出人脸后要画的边框的颜色，RGB格式color = (0, 255, 0)# 将当前帧转换成灰度图像grey = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) 人脸检测并框出人像 读取视频信息后 1234567# 人脸检测，1.2和2分别为图片缩放比例和需要检测的有效点数faceRects = classfier.detectMultiScale(grey, scaleFactor = 1.2, minNeighbors = 1, minSize = (64, 64)) if len(faceRects) &gt; 0: #大于0则检测到人脸 for faceRect in faceRects: #单独框出每一张人脸 x, y, w, h = faceRect cv2.rectangle(frame, (x - 10, y - 10), (x + w + 10, y + h + 10), color, 2) 准备人脸数据 把框出的人脸数据保存为图片 num是图片数 1234# 将当前帧保存为图片img_name = '%s/%d.jpg'%(path_name, num) image = frame[y - 10: y + h + 10, x - 10: x + w + 10]cv2.imwrite(img_name, image) 显示当前捕捉到了多少人脸图片 12font = cv2.FONT_HERSHEY_SIMPLEX cv2.putText(frame,'num:%d' % (num),(x + 30, y + 30), font, 1, (255,0,255),4) 完整代码 地址 效果如下 错误 error: (-215) !empty() in function cv::CascadeClassifier::detectMultiScale 路径错误提示： 找不到你的xml文件，说明你的xml的路径出错了 eg：我的xml在data文件下的data文件中 Keras 2.0版本运行无更新函数出错 Update your Conv2D call to the Keras 2 API 出现这些警告是由于keras的版本更新的问题 Keras 2.0版本中，定义卷积层需要用Conv2D而不再是Convolution2D 即使用 1model.add(Conv2D(64, 3, 3, border_mode='same')) 而不是 1model.add(Convolution2D(64, 3, 3, border_mode='same')) 同时更改import为 1from keras.layers import Conv2D Keras测试错误ProgbarLogger' no attribute 'log_values 由无使用Conv2D函数而引发的错误 描述为：没找到log_values属性 解决方法： 在定义卷积层时使用Conv2D函数，该错误消失 参考资料 吴恩达-神经网络与深度学习-网易云课堂 Neo-T的图像识别系列博客]]></content>
      <categories>
        <category>人脸检测</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
        <tag>人脸检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第四章-深度神经网络优化（梯度下降法）]]></title>
    <url>%2F2018%2F03%2F07%2F%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2Fw%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E4%BC%98%E5%8C%96-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%2F</url>
    <content type="text"><![CDATA[梯度下降法 批梯度下降法（BGD） 批梯度下降法（Batch Gradient Descent，BGD是最常用的梯度下降形式，前面的Logistic回归及深层神经网络的构建中所用到的梯度下降都是这种形式。其在更新参数时使用所有的样本来进行更新，具体过程为： 公式 \[{X = [x^{(1)},x^{(2)},…,x^{(m)}]}\] \[z^{[1]} = w^{[1]}X + b^{[1]}\] \[a^{[1]} = g^{[1]}(z^{[1]})\] \[… \ …\] \[z^{[l]} = w^{[l]}a^{[l-1]} + b^{[l]}\] \[a^{[l]} = g^{[l]}(z^{[l]})\] \[{J(\theta) = \frac{1}{m} \sum_{i=1}^m \mathcal{L}({\hat y}^{(i)}, y^{(i)}) + \frac{\lambda}{2m} \sum\limits_{l=1}^L ||w^{[l]}}||^2_F\] \[{\theta_j:= \theta_j -\alpha\frac{\partial J(\theta)}{\partial \theta_j}}\] 优缺点 优点：最小化所有训练样本的损失函数，得到全局最优解；易于并行实现。 缺点：当样本数目很多时，训练过程会很慢。 随机梯度下降法（SGD） 随机梯度下降法（Stochastic Gradient Descent，SGD）与批梯度下降原理类似，区别在于每次通过一个样本来迭代更新。 #### 公式 \[{X = [x^{(1)},x^{(2)},…,x^{(m)}]}\] \[for\ \ \ i=1,2,…,m\ \{ \ \ \ \ \ \ \ \ \ \ \ \] \[z^{[1]} = w^{[1]}X + b^{[1]}\] \[a^{[1]} = g^{[1]}(z^{[1]})\] \[… \ …\] \[z^{[l]} = w^{[l]}a^{[l-1]} + b^{[l]}\] \[a^{[l]} = g^{[l]}(z^{[l]})\] \[{J(\theta) = \mathcal{L}({\hat y}^{(i)}, y^{(i)}) + \frac{\lambda}{2} \sum\limits_{l=1}^L ||w^{[l]}}||^2_F\] \[\theta_j:= \theta_j -\alpha\frac{\partial J(\theta)}{\partial \theta_j} \}\] mark 优缺点 优点：训练速度快。 缺点：最小化每条样本的损失函数，最终的结果往往是在全局最优解附近，不是全局最优；不易于并行实现。 小批量梯度下降法（MBDG） 小批量梯度下降法（Mini-Batch Gradient Descent，MBGD）是批量梯度下降法和随机梯度下降法的折衷,对用m个训练样本，，每次采用t（1 &lt; t &lt; m）个样本进行迭代更新。具体过程为： 公式 \[{X = [x^{\{1\}},x^{\{2\}},…,x^{\{k = \frac{m}{t}\}}]}\] 其中： \[x^{\{1\}} = x^{(1)},x^{(2)},…,x^{(t)}\] \[x^{\{2\}} = x^{(t+1)},x^{(t+2)},…,x^{(2t)}\] \[… \ …\] 之后： \[for\ \ \ i=1,2,…,k\ \{ \ \ \ \ \ \ \ \ \ \ \ \] \[z^{[1]} = w^{[1]}x^{\{i\}} + b^{[1]}\] \[a^{[1]} = g^{[1]}(z^{[1]})\] \[… \ …\] \[z^{[l]} = w^{[l]}a^{[l-1]} + b^{[l]}\] \[a^{[l]} = g^{[l]}(z^{[l]})\] \[{J(\theta) = \frac{1}{k} \sum_{i=1}^k \mathcal{L}({\hat y}^{(i)}, y^{(i)}) + \frac{\lambda}{2k} \sum\limits_{l=1}^L ||w^{[l]}}||^2_F\] \[\theta_j:= \theta_j -\alpha\frac{\partial J(\theta)}{\partial \theta_j} \}\] 样本数t的值根据实际的样本数量来调整，为了和计算机的信息存储方式相适应，可将t的值设置为2的幂次。将所有的训练样本完整过一遍称为一个epoch。 优缺点 优点：在小批量生产的情况下，通常它优于梯度下降或随机梯度下降（特别是当训练集较大时）。 差异：梯度下降，小批量梯度下降和随机梯度下降之间的差异是您用于执行一个更新步骤的示例数量。 梯度下降优化 指数加权平均 指数加权平均（Exponentially Weight Average）是一种常用的序列数据处理方式，其计算公式为： 公式 \[S_t = \begin{cases} Y_1, &amp; t=1 \\ \beta Y_t + (1-\beta)S_{t-1}, &amp; t&gt;1 \end{cases}\] 其中\(Y_t\)为t下的实际值，\(S_t\)为\(t\)下加权平均后的值，\(\beta\)为权重值。 例子 给定一个时间序列，例如伦敦一年每天的气温值： 其中蓝色的点代表了真实的数据值。 对于一个即时的温度值，取权重值\(\beta\)为0.9，相当于求取之前\(\frac{1}{1 - \beta} = 10\)天的平均值，则有： \[v_0 = 0\] \[v_1 = 0.9v_0 + 0.1\theta_1\] \[… \ …\] \[v_t = 0.9v_{t-1} + 0.1\theta_t\] 根据这些求得的值即得到图中的红色曲线，它反应了温度变化的大致趋势。 当取权重值\(\beta=0.98\)时，可以得到图中更为平滑的绿色曲线。而当取权重值\(\beta=0.5\)时，得到图中噪点更多的黄色曲线。\(\beta\)越大相当于求取平均利用的天数就越多，曲线自然就会越平滑而且越滞后。 求取v_{100}时： \[v_{100} = 0.1*\theta_{100}+0.1*0.9*\theta_{99}+0.1*0.9^2*\theta_{98} \ …\] MOMENTUM梯度下降 动量梯度下降（Gradient Descent with Momentum）是计算梯度的指数加权平均数，并利用该值来更新参数值。具体过程为： 公式 \[v_{dw} = \beta v_{dw} + (1-\beta)dw\] \[v_{db} = \beta v_{dw} + (1-\beta)db\] \[w : = w - \alpha v_{dw}\] \[b := b-\alpha v_{db}\] 速度用零初始化。因此，该算法需要几次迭代才能“建立”速度并开始采取更大步骤 对于b： * 势头越大 \(b\)更新越平滑，因为我们越是将过去的渐变考虑在内。但如果\(b\)它太大了，它也可以使更新变得更顺畅。 * 常见的值 \(b\)范围从0.8到0.999。如果你不想调整它，\(b = 0.9\)通常是一个合理的默认值。 * 调整最佳\(b\) 对于你的模型来说，可能需要尝试几个值来看看在降低成本函数的价值方面什么效果最好 ĴĴ . 特点 动量将渐变考虑在内以平滑梯度下降的步骤。它可以应用分批梯度下降，小批量梯度下降或随机梯度下降。 你必须调整动量超参数 bb 和学习率 RMSPROP算法 RMSProp(Root Mean Square Prop，均方根支)算法还是在对梯度进行指数加权平均的基础上，引入平方和平方根。 公式 \[s_{dw} = \beta s_{dw} + (1-\beta)dw^2\] \[s_{db} = \beta s_{db} + (1-\beta)db^2\] \[w := w-\alpha \frac{dw}{\sqrt{s_{dw}+\epsilon}}\] \[b := b-\alpha \frac{db}{\sqrt{s_{db}+\epsilon}}\] 其中的\(\epsilon=10^{-8}\),提到数值稳定，防止分母太小。 当\(dw\)或\(db\)较大时，\(dw^2\)、\(db^2\)会较大，进而\(s_{dw}\)、 \(s_{db}\)也会较大，最终使得\(\frac{dw}{\sqrt{s_{dw}}}\)、 \(\frac{db}{\sqrt{s_{db}}}\)较小，减少了抵达最小值路径上的摆动。 ADAM优化算法 Adam(Adaptive Moment Estimation，自适应矩估计)优化算法适用于很多不同的深度学习网络结构，它本质上是将Momentum梯度下降和RMSProp算法结合起来。 公式 \[v_{dw} = \beta_1 v_{dw} + (1-\beta_1)dw, \ v_{db} = \beta_1 v_{db} + (1-\beta_1)db\] \[s_{dw} = \beta_2 s_{dw} + (1-\beta_2)dw^2, \ s_{db} = \beta_2 s_{db} + (1-\beta_2)db^2\] \[v^{corrected}_{dw} = \frac{v_{dw}}{(1-\beta_1^t)},\ v^{corrected}_{db} = \frac{v_{db}}{(1-\beta_1^t)}\] \[s^{corrected}_{dw} = \frac{s_{dw}}{(1-\beta_2^t)},\ s^{corrected}_{db} = \frac{s_{db}}{(1-\beta_2^t)}\] \[w := w-\alpha \frac{v^{corrected}_{dw}}{\sqrt{s^{corrected}_{dw}}+\epsilon}\] \[b := b-\alpha \frac{v^{corrected}_{db}}{\sqrt{s^{corrected}_{db}}+\epsilon}\] 其中的学习率\(\alpha\)需要进行调参，超参数\(\beta_1\)被称为第一阶矩，一般取0.9，\(\beta_2\)被称为第二阶矩，一般取0.999，\(\epsilon\)一般取\(10^{-8}\)。 Softmax回归 Softmax回归模型是Logistic回归模型在多分类问题上的推广，在多分类问题中，输出y的值不再是一个数，而是一个多维列向量，有多少种分类是就有多少维数。 公式 softmax函数： \[\sigma(z)_{j}=\frac{e^{z_{j}}}{\sum_{i=1}^m e^{z_{i}}}\] 损失函数也变为： \[\mathcal{L}(a^L, y) = - \sum^m_{i=1}y_i \log a^L\] 代码 课程代码-GitHub 参考资料 吴恩达-神经网络与深度学习-网易云课堂 Deep Learning系列课程资料笔记]]></content>
      <categories>
        <category>深度学习与神经网络(吴恩达)</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第五章-Tensorflow]]></title>
    <url>%2F2018%2F03%2F07%2F%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2Fw%E7%AC%AC%E4%BA%94%E7%AB%A0-Tensorflow%2F</url>
    <content type="text"><![CDATA[Tensorflow 基本流程 步骤 创建尚未执行/评估的张量（变量） 在这些张量之间写入操作 初始化你的张量 创建一个会话 运行会话。这将运行你上面写的操作 eg：初始化你的变量，创建一个会话并在会话中运行这些操作 例子 12345678910# 初始化变量a = tf.constant(2)b = tf.constant(10)c = tf.multiply(a,b)print(c)--&gt;Tensor("Mul_1:0", shape=(), dtype=int32)# 运行会话sess = tf.Session()print(sess.run(c))--&gt;20 会话 方法一： 1234sess = tf.Session()# 运行变量初始化result = sess.run(..., feed_dict = &#123;...&#125;)sess.close() 方法二： 1234with tf.Session() as sess: # 运行变量初始化 result = sess.run(..., feed_dict = &#123;...&#125;) # 自动关闭会话 占位符 占位符只是一个变量（第一次定义时无需为它指定一个值），仅在运行会话时为其分配数据 #### 步骤 * 创建占位符 * 指定与您要计算的操作对应的计算图 * 创建会话 * 运行会话，必要时使用Feed字典来指定占位符变量的值。 例子(tf实现sigmoid函数) 1234567891011def sigmoid(z): # 创建占位符 x = tf.placeholder(tf.float32, name = "x") # 指定与您要计算的操作对应的计算图 sigmoid = tf.sigmoid(x) # 创建会话 with tf.Session() as sess: # 运行会话 result = sess.run(sigmoid, feed_dict = &#123;x:z&#125;) return result ONE-HOT编码（tf.one_hot()） 编码方式如图： #### 代码实现 使用tf.one_hot()实现 123456789101112labels = np.array([1,2,3,0,2,1])one_hot = one_hot_matrix(labels, C = 4)def one_hot_matrix(labels, C): C = tf.constant(C, name="C") one_hot_matrix = tf.one_hot(labels, C, 1) sess = tf.Session() one_hot = sess.run(one_hot_matrix).T sess.close() return one_hot 初始化零和向量（tf.ones()） 12345678910# 创建一个一维数组def ones(shape): ones = tf.ones(shape) sess = tf.Session() ones = sess.run(ones) sess.close() return onesprint ("ones = " + str(ones([3])))--&gt;ones = [ 1. 1. 1.] 代码 课程代码-GitHub 参考资料 吴恩达-神经网络与深度学习-网易云课堂 Deep Learning系列课程资料笔记]]></content>
      <categories>
        <category>深度学习与神经网络(吴恩达)</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第四章-深度神经网络优化（初始化&正则化）]]></title>
    <url>%2F2018%2F03%2F04%2F%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2Fw%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E4%BC%98%E5%8C%96-%E5%88%9D%E5%A7%8B%E5%8C%96%26%E6%AD%A3%E5%88%99%E5%8C%96%2F</url>
    <content type="text"><![CDATA[权重初始化 作用 随机初始化以破坏对称性 加快渐变下降的收敛速度 增加梯度下降收敛到较低训练（和泛化）错误的几率 参数都初始化为零 导致网络无法破坏对称性：这意味着每一层中的每个神经元都会学到相同的值。对于每一层而言，网络并不比线性分类器更强大，如逻辑回归。 随机初始化 将权重初始化为非常大的随机值效果不佳。 对于大的随机值权重，最后一次激活（sigmoid）输出的结果对于某些示例非常接近于0或1，并且当它得到该示例错误时，该示例会导致非常高的损失。 eg： \(log（a[3]）= log(0)\) ，损失达到无限。 初始化不良可能会导致渐变/爆炸渐变，这也会降低优化算法的速度。 如果你长时间训练这个网络，你会看到更好的结果，但用过大的随机数初始化会减慢优化速度。 He initialization(He初始化) 适用于使用ReLU激活的网络 预防过拟合 L2正则化 概述 L2正则化依赖于这样的假设，即具有较小权重的模型比具有较大权重的模型更简单。因此，通过惩罚成本函数中权重的平方值，可以将所有权重驱动到较小的值。 拥有大重量的成本太贵了！这导致更平滑的模型，其中输入变化时输出变化更慢。 特点：L2正则化使您的决策边界更加平滑。如果 λ 太大，也有可能“过度平滑”，从而导致具有高偏差的模型。 运算 成本计算： 正则化项被添加到成本中 \[ E=mc^2\\ {Y = [y^{(1)},y^{(2)},…,y^{(m)}]} \] 在权重矩阵方面，渐变中有额外的术语 权值衰减（Weight Decay）： 权重被推到较小的值。 原理 参数 λ 用来调整式中两项的相对重要程度，较小 λ 偏向于最后使原本的成本函数最小化，较大的λλ偏向于最后使权值w最小化。当λλ较大时，权值w[L]w[L]便会趋近于0，相当于消除深度神经网络中隐藏单元的部分作用。 另一方面，在权值w[L]w[L]变小之下，输入样本X随机的变化不会对神经网络模造成过大的影响，神经网络受局部噪音的影响的可能性变小。这就是正则化能够降低模型方差的原因。 随机失活（DropOut）正则化 梯度检验 概念 梯度检验是用来检测您的反向传播实施是否是正确的 原理 梯度检验的实现原理，是根据导数的定义，对成本函数求导，有： \[J’(\theta) = \frac{\partial J(\theta)}{\partial \theta}= \lim_{\epsilon\rightarrow 0}\frac{J(\theta+\epsilon)-J(\theta-\epsilon)}{2\epsilon}\] 梯度检验公式 \[J’(\theta) = \frac{J(\theta+\epsilon)-J(\theta-\epsilon)}{2\epsilon}\] 其中当ϵϵ越小时，结果越接近真实的导数也就是梯度值。可以使用这种方法，来判断反向传播进行梯度下降时，是否出现了错误。 代码 课程代码-GitHub 参考资料 吴恩达-神经网络与深度学习-网易云课堂 Deep Learning系列课程资料笔记]]></content>
      <categories>
        <category>深度学习与神经网络(吴恩达)</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分析伯乐在线文章数据]]></title>
    <url>%2F2018%2F02%2F15%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%2F%E5%88%86%E6%9E%90%E4%BC%AF%E4%B9%90%E5%9C%A8%E7%BA%BF%E6%96%87%E7%AB%A0%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[一、读取文章数据 pandas读取mysql数据到DataFrame中 1234567891011import pandas as pdfrom sqlalchemy import create_enginedb_info = &#123;'user':'root', 'password':'', 'host':'localhost', 'database':'article_spider' &#125;engine = create_engine('mysql://%(user)s:%(password)s@%(host)s/%(database)s?charset=utf8' % db_info,encoding='utf-8')sql = 'select * from jobbole_article;'df = pd.read_sql(sql , con = engine) 二、数据分析 1. 查看数据 df.info() 查看数据信息 df.isnull() 判断数据是否缺失 #### 2. 清洗数据 只保留title、creat_data、tags三个属性的数据 1df.loc[:,['create_date','title','tags']] 按时间进行排序 1df.sort_values(by='create_date',ascending = False) 将数据类型转换为日期类型并设置为索引 12df['create_date'] = pd.to_datetime(df['create_date']) #将数据类型转换为日期类型df = df.set_index('create_date') # 将dcreate_date设置为索引 获取2017年的文章信息及tags和title内容 123df = df['2017']tags = df['tags']title = df['title'] 3.数据类型转换 首先使用np.array()函数把DataFrame转化为np.ndarray()，再利用tolist()函数把np.ndarray()转为list类型 12345tags_data = np.array(tags)#np.ndarray()tags_list = tags_data.tolist()#listtags_text = "".join(tags_list) # 拼接成texttags_text = tags_text.replace(',','') #把逗号换为空tags_text = tags_text.replace('/','') 4.中文分词 利用结巴分词进行中文分词操作 123456import jieba import pandas as pd jieba.add_word('C/C++')segment = jieba.lcut(tags_text)words_df = pd.DataFrame(&#123;'segment':segment&#125;)words_df.head() 进行词频统计 1234import numpywords_stat = words_df.groupby(by=['segment'])['segment'].agg(&#123;"计数":np.size&#125;)words_stat = words_stat.reset_index().sort_values(by=["计数"],ascending=False)words_stat.head() #### 5. 词云显示数据 123456789101112import matplotlib.pyplot as plt%matplotlib inlineimport matplotlibmatplotlib.rcParams['figure.figsize'] = (10.0, 5.0)from wordcloud import WordCloud#词云包#用词云进行显示wordcloud=WordCloud(font_path="simhei.ttf",background_color="white",max_font_size=80)word_frequence = &#123;x[0]:x[1] for x in words_stat.head(1000).values&#125;wordcloud = wordcloud.fit_words(word_frequence)plt.imshow(wordcloud) 得到关于伯乐在线2017年的文章的标签的使用程度如下]]></content>
      <categories>
        <category>数据挖掘</category>
      </categories>
      <tags>
        <tag>数据挖掘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib画图(基础)]]></title>
    <url>%2F2018%2F01%2F28%2FPython%E5%9F%BA%E7%A1%80%2Fmatplotlib%E7%94%BB%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[基本使用 基本格式 123456789import matplotlib.pyplot as pltimport numpy as npx = np.linspace(-1, 1, 50)y = 2*x + 1plt.figure(num=3, figsize=(8, 5),)plt.plot(x, y)plt.show() np.linspace：定义x：范围是(-1,1);个数是50. 仿真一维数据组(x ,y)表示曲线1. plt.figure：定义一个图像窗口，小窗口里面还可以有更多的小图片。这里定义一个图像窗口：编号为3；大小为(8, 5) plt.plot：画(x ,y)曲线. plt.show：显示图像. ### 基础设置 #### 图像坐标轴设置 12345plt.plot(x, y1, color='red', linewidth=1.0, linestyle='--')plt.xlim((-1, 2))plt.ylim((-2, 3))plt.xlabel('I am x')plt.ylabel('I am y') plot中对图像进行设置，如color(颜色),linewidth(行宽),linestyle(曲线的类型) xlim：x坐标轴范围 ylim：y坐标轴范围 xlabel：x坐标轴名称 ylabel：y坐标轴名称 123new_ticks = np.linspace(-1, 2, 5)plt.xticks(new_ticks)plt.yticks([-2, -1.8, -1, 1.22, 3],[r'$really\ bad$', r'$bad$', r'$normal$', r'$good$', r'$really\ good$']) xticks：设置x轴刻度 yticks：设置y轴刻度以及名称：刻度为[-2, -1.8, -1, 1.22, 3]；对应刻度的名称为[‘really bad’,’bad’,’normal’,’good’, ‘really good’]. 图像边框 123ax = plt.gca()ax.spines['right'].set_color('none')ax.spines['top'].set_color('none') gca：获取当前坐标轴信息. spines：设置边框 set_color：设置边框颜色：默认白色； 改变坐标轴的位置 12ax.xaxis.set_ticks_position('bottom') # 设置x坐标刻度数字或名称的位置ax.spines['bottom'].set_position(('data', 0)) # 设置边框位置：x=0/y=0的位置 xaxis.set_ticks_position：设置x坐标刻度数字或名称的位置 bottom.（所有位置：top，bottom，both，default，none），默认是bottom spines：设置边框：x轴； set_position：设置边框位置：x=0/y=0的位置；（位置所有属性：outward，axes，data） ('data', 0)：标识0的位置，让x和y轴的位置移动到此 12ax.yaxis.set_ticks_position('left')ax.spines['left'].set_position(('data',0)) mark Legend 图例 基本配置 123456789101112131415import matplotlib.pyplot as pltimport numpy as npx = np.linspace(-3, 3, 50)y1 = 2*x + 1y2 = x**2plt.figure()plt.xlim((-1, 2))plt.ylim((-2, 3))new_ticks = np.linspace(-1, 2, 5)plt.xticks(new_ticks)plt.yticks([-2, -1.8, -1, 1.22, 3],[r'$really\ bad$', r'$bad$', r'$normal$', r'$good$', r'$really\ good$']) 绘制图例 123l1, = plt.plot(x, y1, label='linear line')l2, = plt.plot(x, y2, color='red', linewidth=1.0, linestyle='--', label='square line')plt.legend(loc='upper right') label：图例名称 loc='upper right'：表示图例将添加在图中的右上角. 调整label 重新设置了线条对应的 label 1plt.legend(handles=[l1, l2], labels=['up', 'down'], loc='best') handles=[l1, l2]：指上面代码中的l1, l2,（要以逗号结尾，因为plt.plot()返回的是一个列表.） best：表示自动分配最佳位置 ioc参数 1234567891011'best' : 0, 'upper right' : 1,'upper left' : 2,'lower left' : 3,'lower right' : 4,'right' : 5,'center left' : 6,'center right' : 7,'lower center' : 8,'upper center' : 9,'center' : 10, Annotation 标注 绘制图像基本信息 1234567891011121314151617181920212223import matplotlib.pyplot as pltimport numpy as npx = np.linspace(-3, 3, 50)y = 2*x + 1plt.figure(num=1, figsize=(8, 5),)plt.plot(x, y,)ax = plt.gca()ax.spines['right'].set_color('none')ax.spines['top'].set_color('none') ax.xaxis.set_ticks_position('bottom') # 设置x坐标刻度数字或名称的位置ax.spines['bottom'].set_position(('data', 0)) # 设置边框位置：x=0/y=0的位置ax.yaxis.set_ticks_position('left')ax.spines['left'].set_position(('data', 0))# 画一条垂直于x轴的虚线.x0 = 1y0 = 2*x0 + 1plt.plot([x0, x0,], [0, y0,], 'k--', linewidth=2.5)# 设置点的类型plt.scatter([x0, ], [y0, ], s=50, color='b') 添加注释 annotate 123plt.annotate(r'$2x+1=%s$' % y0, xy=(x0, y0), xycoords='data', xytext=(+30, -30), textcoords='offset points', fontsize=16, arrowprops=dict(arrowstyle='-&gt;', connectionstyle="arc3,rad=.2")) xycoords='data'：基于数据的值来选位置 xytext=(+30, -30)： xy 的偏差值 textcoords='offset points：对于标注位置的描述 arrowprops：对图中箭头类型的一些设置 添加注释 text 12plt.text(-3.7, 3, r'$This\ is\ the\ some\ text. \mu\ \sigma_i\ \alpha_t$', fontdict=&#123;'size': 16, 'color': 'r'&#125;) -3.7, 3,是选取text的位置 空格需要用到转字符\ fontdict设置文本字体 tick 能见度(透明度) 绘制图像基本信息 123456789101112131415161718import matplotlib.pyplot as pltimport numpy as npx = np.linspace(-3, 3, 50)y = 0.1*xplt.figure()# 在 plt 2.0.2 或更高的版本中, 设置 zorder 给 plot 在 z 轴方向排序plt.plot(x, y, linewidth=10, zorder=1)plt.ylim(-2, 2)ax = plt.gca()ax.spines['right'].set_color('none')ax.spines['top'].set_color('none')ax.spines['top'].set_color('none')ax.xaxis.set_ticks_position('bottom')ax.spines['bottom'].set_position(('data', 0))ax.yaxis.set_ticks_position('left')ax.spines['left'].set_position(('data', 0)) mark 调整坐标 对被遮挡的图像调节相关透明度，本例中设置 x轴 和 y轴 的刻度数字进行透明度设置 1234for label in ax.get_xticklabels() + ax.get_yticklabels(): label.set_fontsize(12) # 在 plt 2.0.2 或更高的版本中, 设置 zorder 给 plot 在 z 轴方向排序 label.set_bbox(dict(facecolor='white', edgecolor='None', alpha=0.7, zorder=2)) zorder：图层的显示顺序 set_fontsize(12)：重新调节字体大小 set_bbox：设置目的内容的透明度相关参 facecolor调节 box 前景色，edgecolor 设置边框， 本处设置边框为无，alpha设置透明度 参考 莫烦PYTHON]]></content>
      <categories>
        <category>Python基础</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib画图(进阶)]]></title>
    <url>%2F2018%2F01%2F28%2FPython%E5%9F%BA%E7%A1%80%2Fmatplotlib%E7%94%BB%E5%9B%BE-%E8%BF%9B%E9%98%B6%2F</url>
    <content type="text"><![CDATA[多图合一显示 Subplot 多合一显示 均匀图中图 1234567891011import matplotlib.pyplot as pltplt.figure()plt.subplot(1,2,1)plt.plot([0,1],[0,1])plt.subplot(1,2,2)plt.plot([0,1],[0,2])plt.show() # 展示 plt.figure：定义一个图像窗口，小窗口里面还可以有更多的小图片。 plt.subplo(x, y, n)：表示将整个图像窗口分为x行y列,当前位置为n 可简写为plt.subplo(xyn) plt.plot：画(x ,y)曲线. 参考 莫烦PYTHON]]></content>
      <categories>
        <category>Python基础</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[章三习题]]></title>
    <url>%2F2018%2F01%2F22%2F%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2Fw%E7%AB%A0%E4%B8%89%E4%B9%A0%E9%A2%98%2F</url>
    <content type="text"><![CDATA[导入包 numpy是用Python进行科学计算的基本软件包。 sklearn为数据挖掘和数据分析提供了简单高效的工具。 matplotlib是一个用于在Python中绘制图表的库。 testCases提供了一些测试例子来评估你的函数的正确性 planar_utils提供了在这个任务中使用的各种有用的功能 1234567891011import numpy as npimport matplotlib.pyplot as pltfrom testCases import * import sklearn import sklearn.datasetsimport sklearn.linear_modelfrom planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets%matplotlib inlinenp.random.seed(1) # 设置seed，使结果一致 导入“花卉数据集” 加载一个“花” 2级的数据集中到变量X和Y。 1X, Y = load_planar_dataset() 使用matplotlib可视化数据集。 数据看起来像一朵红色（标签y = 0）和一些蓝色（y = 1）点的“花朵”。你的目标是建立一个模型来适应这个数据。 1plt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral); mark 练习 你有几个训练样例？此外，什么是shape变量X和Y？ shape属性通常用于获取数组的当前形状，但也可用于通过为其分配数组维数来重新定形数组。 1234567shape_X = X.shape shape_Y = Y.shapem = shape_X[1] # training set sizeprint ('The shape of X is: ' + str(shape_X))print ('The shape of Y is: ' + str(shape_Y))print ('I have m = %d training examples!' % (m)) 简单的logistic逻辑回归 使用sklearn的内置函数来训练数据集上的logistic回归分类器。 12clf = sklearn.linear_model.LogisticRegressionCV();clf.fit(X.T, Y.T); 绘制逻辑回归的决策边界 12plot_decision_boundary(lambda x: clf.predict(x), X, Y)plt.title("Logistic Regression") 打印精度 123LR_predictions = clf.predict(X.T)print ('逻辑回归的准确性：%d ' % float((np.dot(Y,LR_predictions) + np.dot(1-Y,1-LR_predictions))/float(Y.size)*100) + '% ' + "（正确标记的数据点的百分比）") 注：数据集不是线性可分的，所以逻辑回归表现不好。希望神经网络能做得更好。 神经网络模型 构建神经网络模型的一般方法 定义输入单位，隐藏单位等的神经网络结构。 初始化模型的参数 循环： - 3.1 实施前向传播 - 3.2 计算损失 - 3.3 实现向后传播以获得渐变 - 3.4 更新参数（梯度下降） 模型 定义神经网络结构 定义三个变量： - n_x: the size of the input layer - n_h: the size of the hidden layer (set this to 4) - n_y: the size of the output layer 123456# 分级功能def layer_sizes(X, Y): n_x = X.shape[0] # size of input layer n_h = 4 n_y = Y.shape[0] # size of output layer return (n_x, n_h, n_y) 12345X_assess, Y_assess = layer_sizes_test_case()(n_x, n_h, n_y) = layer_sizes(X_assess, Y_assess)print("The size of the input layer is: n_x = " + str(n_x))print("The size of the hidden layer is: n_h = " + str(n_h))print("The size of the output layer is: n_y = " + str(n_y)) 预计输出 初始化模型的参数 实现该功能initialize_parameters() * 用随机值初始化权重矩阵。 使用：np.random.randn(a,b) * 0.01随机初始化形状矩阵（a，b）。 * 初始化偏置向量为零。 使用：np.zeros((a,b))用零初始化形状矩阵（a，b）。 12345678910111213141516171819202122232425262728def initialize_parameters(n_x, n_h, n_y): """ Returns: params -- python dictionary containing your parameters: W1 -- weight matrix of shape (n_h, n_x) - 形状的权重矩阵 b1 -- bias vector of shape (n_h, 1) - 形状的偏向量 W2 -- weight matrix of shape (n_y, n_h) - 形状权重矩阵 b2 -- bias vector of shape (n_y, 1) - 形状的偏向量 """ np.random.seed(2) W1 = np.random.randn(n_h,n_x) * 0.01 #随机初始化 b1 = np.zeros((n_h,1)) W2 = np.random.randn(n_y,n_h) * 0.01 #随机初始化 b2 = np.zeros((n_y,1)) assert (W1.shape == (n_h, n_x)) assert (b1.shape == (n_h, 1)) assert (W2.shape == (n_y, n_h)) assert (b2.shape == (n_y, 1)) parameters = &#123;"W1": W1, "b1": b1, "W2": W2, "b2": b2&#125; return parameters 1234567n_x, n_h, n_y = initialize_parameters_test_case()parameters = initialize_parameters(n_x, n_h, n_y)print("W1 = " + str(parameters["W1"]))print("b1 = " + str(parameters["b1"]))print("W2 = " + str(parameters["W2"]))print("b2 = " + str(parameters["b2"])) 循环 前向传播 123456789101112131415161718192021222324252627282930def forward_propagation(X, parameters): """ 论据: X -- 输入数据大小 (n_x, m) parameters -- 包含你的参数的python字典 (初始化函数的输出) 返回: A2 -- The sigmoid output of the second activation 第二次激活的s的输出 cache -- a dictionary containing "Z1", "A1", "Z2" and "A2" """ # 从字典“参数”中检索每个参数 W1 = parameters["W1"] b1 = parameters["b1"] W2 = parameters["W2"] b2 = parameters["b2"] # 实现正向传播来计算A2（概论） Z1 = np.dot(W1, X)+b1 A1 = np.tanh(Z1) Z2 = np.dot(W2, A1)+b2 A2 = sigmoid(Z2) assert(A2.shape == (1, X.shape[1])) cache = &#123;"Z1": Z1, "A1": A1, "Z2": Z2, "A2": A2&#125; return A2, cache 计算成本函数 12345678#计算成本函数def compute_cost(A2, Y, parameters): m = Y.shape[1] cost = -np.sum(np.multiply(Y,np.log(A2)) + np.multiply((1-Y),np.log(1-A2)))/m cost = np.squeeze(cost) return cost 反向传播 12345678910111213141516171819202122def backward_propagation(parameters, cache, X, Y): m = X.shape[1] # 从字典“parameters”中检索W1和W2 W1 = parameters["W1"] W2 = parameters["W2"] # 从字典“cache”中检索W1和W2 A1 = cache["A1"] A2 = cache["A2"] # 反向传播：计算dW1, db1, dW2, db2 dZ2 = A2 -Y dW2 = np.dot(dZ2, A1.T)/m db2 = np.sum(dZ2, axis=1, keepdims=True)/m dZ1 = np.dot(W2.T, dZ2)*(1 - np.power(A1,2)) #g'(x) = 1-(g(x))^2 dW1 = np.dot(dZ1, X.T)/m db1 = np.sum(dZ1, axis=1, keepdims=True)/m grads = &#123;"dW1":dW1, "db1":db1, "dW2":dW2, "db2":db2&#125; return grads 更新参数 12345678910111213141516171819202122def update_parameters(parameters, grads, learning_rate = 1.2): # 从字典“parameters”中检索W1和W2 W1 = parameters["W1"] b1 = parameters["b1"] W2 = parameters["W2"] b2 = parameters["b2"] # 从字典中检索每个渐变“grads” dW1 = grads["dW1"] db1 = grads["db1"] dW2 = grads["dW2"] db2 = grads["db2"] # 更新每个参数的规则 W1 = W1 - learning_rate*dW1 b1 = b1 - learning_rate*db1 W2 = W2 - learning_rate*dW2 b2 = b2 - learning_rate*db2 parameters = &#123;"W1": W1, "b1": b1, "W2": W2, "b2": b2&#125; return parameters 建立神经网络模型 1234567891011121314151617181920212223def nn_model(X, Y, n_h, num_iterations = 10000, print_cost = False): np.random.seed(3) n_x = layer_sizes(X, Y)[0] n_y = layer_sizes(X, Y)[2] # 初始化参数，输入n_x, n_h, n_y，输出W1, b1, W2, b2，参数 parameters = initialize_parameters(n_x, n_h, n_y); W1 = parameters["W1"] b1 = parameters["b1"] W2 = parameters["W2"] b2 = parameters["b2"] for i in range(0, num_iterations): # 前向传播 A2, cache = forward_propagation(X, parameters) cost = compute_cost(A2, Y, parameters) grads = backward_propagation(parameters, cache, X, Y) parameters = update_parameters(parameters, grads) if print_cost and i % 1000 == 0: print("循环%i次后的成本: %f" %(i, cost)) return parameters 预测结果 使用您的模型通过构建predict（）来进行预测。使用前向传播来预测结果 123456def predict(parameters, X): # 使用正向传播计算概论，并使用0.5作为阈值分类为0/1 A2, cache= forward_propagation(X, parameters) predictions = (A2 &gt; 0.5) return predictions 将数据输入神经网络模型 用一个隐藏层测试你的模型 ñHñH 隐藏单位 12345678910# 建立一个n_h维隐藏层的模型parameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)# 绘制决策边界plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)plt.title("Decision Boundary for hidden layer size " + str(4))# 打印精度predictions = predict(parameters, X)print ('准确度: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%') #打印精确度 调整隐藏层大小（可选） 观察模型的各种隐藏层大小的不同行为 12345678910plt.figure(figsize=(16, 32))hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50, 100]for i, n_h in enumerate(hidden_layer_sizes): plt.subplot(5, 2, i+1) plt.title('Hidden Layer of size %d' % n_h) parameters = nn_model(X, Y, n_h, num_iterations = 5000) plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y) predictions = predict(parameters, X) accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) print ("节点数为&#123;&#125;时的分类准确度为 : &#123;&#125; %".format(n_h, accuracy)) 较大的模型（具有更多的隐藏单元）能够更好地适应训练集，直到最终的最大模型过度拟合数据。 最好的隐藏层大小似乎在n_h = 5左右。事实上，这里的一个值似乎很适合数据，而且不会引起显着的过度拟合。 代码 课程代码-GitHub 参考资料 吴恩达-神经网络与深度学习-网易云课堂 Deep Learning系列课程资料笔记]]></content>
      <categories>
        <category>深度学习与神经网络(吴恩达)</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy爬虫一]]></title>
    <url>%2F2018%2F01%2F16%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%2FScrapy%E7%88%AC%E8%99%AB%E4%B8%80%2F</url>
    <content type="text"><![CDATA[理解业务 画出爬取网站的url结构图 ### 爬取方式 #### 深度优先 爬虫去重实现破除循环（深度优先） #### 通过翻页爬取 初始Scrapy项目 创建项目 在对应的目录下的cmd中输入scrapy startproject ArticleSpider创建项目 进入项目目录 cd ArticleSpider 输入scrapy genspider jobbole blog.jobbole.com创建爬虫module setting.py中如下修改 12# 关闭过滤不符合ROBOTSTXT条例的url的功能ROBOTSTXT_OBEY = False 调试项目 在项目目录文件下创建main.py文件专门用来调试项目爬虫 123456789import sysimport osfrom scrapy.cmdline import execute# 获取项目当前位置address = sys.path.append(os.path.dirname(os.path.abspath(__file__)))print(address)# 用execute调用命令打开爬虫，cmd中命令为`scrapy crawl jobbole`execute(["scrapy", "crawl", "jobbole"]) 调试单个网址 在cmd中输入scrapy shell url,url为目标地址 开始调试，如下 获取网页内容 XPath 概念 XPath 是一门在xml文档中查找信息的语言，它可以在XML文档中对于原色和属性进行遍历。其内置了超过100个内建函数，这些函数用于对字符串值，数值、日期、时间进行比较遍历。 #### 基本使用语法 #### 两种查找方式 ##### 通过源码查找 ##### 通过浏览器开发者模式直接复制得到xpath位置 #### 提取内容信息 通过a/text()提取文字信息，通过a.extract()提取为数组，调用时只需调用数组的值 注：调用extract()后信息存储为数组形式，selector无法继续查询子条件 如a/div不可用 #### 爬取一篇文章的内容 123456789101112 #提取文章的具体字段#extract_first()提取第一个数组元素，默认为空，可防止空异常title = response.xpath('//div[@class="entry-header"]/h1/text()').extract_first("")# strip()去除空格 replace()把 · 替换为空create_date = response.xpath("//p[@class='entry-meta-hide-on-mobile']/text()").extract()[0].strip().replace("·","").strip()# xpath中的contains()函数获取包含该字符串的该元素的classpraise_nums = response.xpath("//span[contains(@class, 'vote-post-up')]/h10/text()").extract()[0]fav_nums = response.xpath("//span[contains(@class, 'bookmark-btn')]/text()").extract()[0]# re模块的正则表达式获取数值match_re = re.match(".*?(\d+).*", fav_nums)if match_re: fav_nums = match_re.group(1) CSS选择器 123456789101112131415161718192021222324# 通过css选择器提取字段 front_image_url = response.meta.get("front_image_url", "") #文章封面图 title = response.css(".entry-header h1::text").extract()[0] create_date = response.css("p.entry-meta-hide-on-mobile::text").extract()[0].strip().replace("·","").strip() praise_nums = response.css(".vote-post-up h10::text").extract()[0] fav_nums = response.css(".bookmark-btn::text").extract()[0] match_re = re.match(".*?(\d+).*", fav_nums) if match_re: fav_nums = int(match_re.group(1)) else: fav_nums = 0 comment_nums = response.css("a[href='#article-comment'] span::text").extract()[0] match_re = re.match(".*?(\d+).*", comment_nums) if match_re: comment_nums = int(match_re.group(1)) else: comment_nums = 0 content = response.css("div.entry").extract()[0] tag_list = response.css("p.entry-meta-hide-on-mobile a::text").extract() tag_list = [element for element in tag_list if not element.strip().endswith("评论")] tags = ",".join(tag_list) XPath与CSS选择器的区别 CSS选择器编码比较简短，适合熟悉前端的人员使用，两者都可以采集到网站的信息 获取文章url列表(翻页) 通过request函数调用parse_detail方法，通过parse函数拼接url 12345678910111213141516171819202122def parse(self, response): """ 1. 获取文章列表页中的文章url并交给scrapy下载后并进行解析 2. 获取下一页的url并交给scrapy进行下载， 下载完成后交给parse """ # 解析列表页中的所有文章url并交给scrapy下载后并进行解析 if response.status == 404: self.fail_urls.append(response.url) self.crawler.stats.inc_value("failed_url") post_nodes = response.css("#archive .floated-thumb .post-thumb a") for post_node in post_nodes: # 获取文章封面 image_url = post_node.css("img::attr(src)").extract_first("") post_url = post_node.css("::attr(href)").extract_first("") # request函数调用parse_detail方法，parse函数拼接url，meta传递的是字典做异步处理把图片传给下一层 yield Request(url=parse.urljoin(response.url, post_url), meta=&#123;"front_image_url": image_url&#125;, callback=self.parse_detail) # 提取下一页并交给scrapy进行下载,调用自身 next_url = response.css(".next.page-numbers::attr(href)").extract_first("") if next_url: yield Request(url=parse.urljoin(response.url, post_url), callback=self.parse) 定义Item中的类 Spider中的Item只有一种类型Field，表示可以接收任何类型 12345678910111213# 只有一种类型Fieldclass JobBoleAeticleItem(scrapy.Item): title = scrapy.Field() create_date = scrapy.Field() url = scrapy.Field() url_object_id = scrapy.Field() # 把url的长度变成固定长度 front_image_url = scrapy.Field() front_image_path = scrapy.Field() praise_nums = scrapy.Field() comment_nums = scrapy.Field() fav_nums = scrapy.Field() tags = scrapy.Field() content = scrapy.Field() 在jobbole中填充传值 123456789101112131415161718# 填充传值 article_item["url_object_id"] = get_md5(response.url) article_item["title"] = title article_item["url"] = response.url try: create_date = datetime.datetime.strptime(create_date, "%Y/%m/%d").date() except Exception as e: create_date = datetime.datetime.now().date() article_item["create_date"] = create_date article_item["front_image_url"] = [front_image_url] article_item["praise_nums"] = praise_nums article_item["comment_nums"] = comment_nums article_item["fav_nums"] = fav_nums article_item["tags"] = tags article_item["content"] = content # 把值传到pipelines中 yield article_item settings 开启pipelines默认配置 在settings.py中把默认设置的 123ITEM_PIPELINES = &#123; 'ArticleSpider.pipelines.ArticlespiderPipeline': 300,&#125; scrapy自带的图片下载 1234567891011ITEM_PIPELINES = &#123; 'scrapy.pipelines.images.ImagesPipeline': 1,&#125;IMAGES_URLS_FIELD = "front_image_url"# abspath获取当前目录 dirname获取目录名称 join拼接project_dir = os.path.abspath(os.path.dirname(__file__))IMAGES_STORE = os.path.join(project_dir, 'images')# 设置选择要下载图片的规模# IMAGES_MIN_HEIGHT = 100# IMAGES_MIN_WIDTH = 100 继承ImagesPipeline获取图片地址 12345678910from scrapy.pipelines.images import ImagesPipelineclass ArticleImagePipeline(ImagesPipeline): # results中保存的是图片的路径，是list形式 def item_completed(self, results, item, info): if "front_image_url" in item: for ok, value in results: image_file_path = value["path"] item["front_image_path"] = image_file_path return item 固定url格式 新建一个名为utils的文件 12345678def get_md5(url): # 如果是unicode,转为utf-8 if isinstance(url, str): url = url.encode("utf-8") m = hashlib.md5() m.update(url) # 抽取返回摘要 return m.hexdigest() 数据写入sql josn文件导出基本格式 codecs：这样用到codecs来打开文件，优点是可以避免打开过程中出现的一些问题 123456789101112class JsonWithEncodingPipeline(object): #自定义json文件的导出 def __init__(self): self.file = codecs.open('articleexport.json', 'w', encoding='utf-8') def process_item(self, item, spider): lines = json.dumps(dict(item), ensure_ascii=False) + "\n" self.file.write(lines) return item def close_spider(self, spider): self.file.close() 调用scrapy提供的json export导出json文件 1234567891011121314class JsonExporterPipleline(object): #调用scrapy提供的json export导出json文件 def __init__(self): self.file = open('articleexport.json', 'wb') self.exporter = JsonItemExporter(self.file, encoding="utf-8", ensure_ascii=False) self.exporter.start_exporting() def process_item(self, item, spider): self.exporter.export_item(item) return item def close_spider(self, spider): self.exporter.finish_exporting() self.file.close() 设计数据库 create_date日期数据的设计 jobbole.py中调用datetime类把数据改为日期形式 1234try: create_date = datetime.datetime.strptime(create_date, "%Y/%m/%d").date()except Exception as e: create_date = datetime.datetime.now().date() 各种数据 #### 安装mysqlclient windows下pip install mysqlclinet MysqlPipeline setting下配置MYSQL 12345678MYSQL_HOST = "127.0.0.1"MYSQL_DBNAME = "article_spider"MYSQL_USER = "root"MYSQL_PASSWORD = "root"SQL_DATETIME_FORMAT = "%Y-%m-%d %H:%M:%S"SQL_DATE_FORMAT = "%Y-%m-%d" MysqlTwistedPipline(异步插入) 简化代码 通过scrapy的item loader传值 主要语法 item_loader.add_css( ) item_loader.add_xpath( ) item_loader.add_value( ) 12item_loader = ItemLoader(item=, response=response) item_loader.add_css() 传值 123456789101112# 通过item loader加载item item_loader = ItemLoader(item=JobBoleArticleItem(), response=response) item_loader.add_css("title", ".entry-header h1::text") item_loader.add_value("url", response.url) item_loader.add_value("url_object_id", get_md5(response.url)) item_loader.add_css("create_date", "p.entry-meta-hide-on-mobile::text") item_loader.add_value("front_image_url", [front_image_url]) item_loader.add_css("praise_nums", ".vote-post-up h10::text") item_loader.add_css("comment_nums", "a[href='#article-comment'] span::text") item_loader.add_css("fav_nums", ".bookmark-btn::text") item_loader.add_css("tags", "p.entry-meta-hide-on-mobile a::text") item_loader.add_css("content", "div.entry") 通过scrapy的自定义的Item提取信息 MapCompose方法 12345678910111213141516171819class JobBoleArticleItem(scrapy.Item): create_date = scrapy.Field( input_processor=MapCompose(date_convert), # output_processor=TakeFirst() 获取第一个 ) url = scrapy.Field() # 把url的长度变成固定长度 url_object_id = scrapy.Field() front_image_url = scrapy.Field( # 用TakeFirst()会出异常 output_processor=MapCompose(return_value) ) front_image_path = scrapy.Field() praise_nums = scrapy.Field( input_processor=MapCompose(get_nums) ) comment_nums = scrapy.Field( input_processor=MapCompose(get_nums) ) 自定义]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux基础]]></title>
    <url>%2F2018%2F01%2F16%2Flinux%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[配置 安装Mysql linux下输入 sudo apt-get install mysql-server安装 用ps aux|grep mysqld检查mysql是否运行成功 musql -uroot -p登录 登录后 show databases;显示表信息 exit 关闭 打开## 配置]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[好用的软件]]></title>
    <url>%2F2018%2F01%2F15%2FA%E6%9C%89%E7%94%A8%E7%9A%84%E4%B8%9C%E8%A5%BF%2F</url>
    <content type="text"><![CDATA[下载相关 Free Download Manager : 比迅雷9(浏览器)好用无数倍的下载软件 百度云破解版：网上有教程，可从网页版的百度云资源下载的地方获取下载地址，然后用别的下载软件下载 油猴插件：各种功能 b站视频下载插件：greasyfork.org(https://greasyfork.org/zh-CN/scripts/by-site/bilibili.com?site=bilibili.com&amp;q=%E8%A7%86%E9%A2%91%E4%B8%8B%E8%BD%BD) 游戏 夜神模拟器：平时玩个小游戏，安卓捣鼓一些东西蛮适用的 办公 TIM : 摆脱低级趣味还你清净的qq版本，适合办公使用 福晰阅读器：界面我很喜欢，比WPS的界面好 编程相关： github for Windows：简化对github的使用的一个客户端版本，手残党的福利 navicat ：轻量级SQL管理工具 MPic： 一个图床工具，设置好七牛云后实现一键复制、上传图片 谷歌插件 谷歌访问助手: 安装后即可访问Google（可登陆Google账号、Google搜索和访问Google商店） Greenhub Socks Proxy：翻墙工具（学术研究用，与谷歌访问助手不能共存） 下载+ : 优化谷歌浏览器中显示下载的内容 ImTranslator: 翻译，字典，声音]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sublime基础]]></title>
    <url>%2F2018%2F01%2F15%2Fsublime%E4%B8%80%2F</url>
    <content type="text"><![CDATA[配置 主题 快捷键 搜索类 Ctrl+F 适合搜索当前文件中的关键字 Ctrl+R 适合搜索当前文件中的函数名 Ctrl+P 可以搜索当前项目的文件名，方便快速找到想找的文件 Ctrl+Shift+P 打开命令框，eg：输入关键字，调用sublime text或插件的功能，例如使用package安装插件。 自定义快捷键 在浏览器打开 12345678&#123; "keys": ["f1"], "command": "side_bar_files_open_with", "args": &#123; "paths": [], "application": "C://Users//Administrator//AppData//Local//Google//Chrome//Application//chrome.exe", "application": "", "extensions":".*" &#125; &#125;, 打开/隐藏侧边栏 原快捷键为 { &quot;keys&quot;: [&quot;ctrl+k&quot;, &quot;ctrl+b&quot;], &quot;command&quot;: &quot;toggle_side_bar&quot; }, 极不方便，改为f2 参考 Sublime Text 3 快捷键汇总]]></content>
      <categories>
        <category>Sublime</category>
      </categories>
      <tags>
        <tag>Sublime</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[识别手机视频中的文字信息]]></title>
    <url>%2F2018%2F01%2F13%2F%E8%AF%86%E5%88%AB%E6%89%8B%E6%9C%BA%E8%A7%86%E9%A2%91%E4%B8%AD%E7%9A%84%E6%96%87%E5%AD%97%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[主要流程如下 ## 利用adb进行手机截图 ### pc端安装adb ### 连接到手机 #### 通过wifi(root) #### 通过usb设备(未root) 可能出现的问题 由于目标计算机积极拒绝，无法连接 在usb连接下 输入命令 adb tcpip 5555 然后再进行连接 adb connect ip ### 进入 shell 模式 只有一个连接设备 adb shell 多个连接设备 adb -s &lt;serial number&gt; shell &lt;serial number&gt;是连接设备的名称]]></content>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘-特征工程]]></title>
    <url>%2F2018%2F01%2F05%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[特征抽象 时间戳 选择合适的时间 把年月日格式的数据转化为十进制 二值类 将数据量化 多值有序类 多值无序类 信息阉割 One-hot 编码 One-hot编码方式的优点是保存了所以信息的信息量 原则：是通过唯一数值标识每个字符数据在其特征列中的位置属性来实现特征编码 ### 文本类型 ### 图像或语音数据 特征重要性评估（信息在模型中所占的权重） 回归模型系数判断法（通过最优算法得到参数的系数） 注：想要通过逻辑回归的模型各特征参数来评估特征的重要性需要对数据进行=归一化=处理（除去量纲对于数据的影响） ### 信息熵判断法 #### 信息熵 #### 信息增益 特征衍生]]></content>
      <categories>
        <category>数据挖掘</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>数据挖掘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdowm总结]]></title>
    <url>%2F2018%2F01%2F04%2Fmarkdowm%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[基本概念 在Sublime中预览markdown环境 通过Markdown Preview插件 按Ctrl + Shift + P，输入install Packages回车 ＊ 输入Markdown Preview回车，进行安装 设置预览快捷键 在Preferences的Key Bindings中设置快捷键alt+m 命令为preview in browser，意思是可以在浏览器看到预览效果 1&#123; "keys": ["alt+m"], "command": "markdown_preview", "args": &#123; "target": "browser"&#125; &#125; 实时预览 安装Markmon：real-time markdown preview 安装：按Ctrl + Shift + P，输入install Packages回车 ＊ 输入Markmon回车，进行安装 npm 安装 markmon (npm)； npm install -g markmon 安装 pandoc 提供一个pandoc-windows.msi 下载地址 pandoc: a universal document converter. 这个转换器负责 markdown 语法转到 html 语法，实时监控文件编辑，以便实时预览； 配置 Package Settings Preferences-&gt;Package Settings-&gt;Markmon-&gt;Settings - User，编辑文件如下： { &quot;executable&quot;: &quot;markmon.cmd&quot;, } 重启 Sublime； 使用： 通过菜单 Tools &gt; markmon &gt; Launch，或者 Control+Shift+P：Markmon launch。 在浏览器会打开 localhost:3002 页面，对于 Sublime 中的改变，你就会在浏览器中实时看到。]]></content>
      <categories>
        <category>markdowm</category>
      </categories>
      <tags>
        <tag>markdowm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘-数据预处理]]></title>
    <url>%2F2018%2F01%2F03%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[采样 归一化 去噪声]]></content>
      <categories>
        <category>数据挖掘</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>数据挖掘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MATLAB-Simulink]]></title>
    <url>%2F2018%2F01%2F03%2FMATLAB%2FMATLAB-Simulink%20%2F</url>
    <content type="text"><![CDATA[Simulink 用 Simulink 求解黄金分割比]]></content>
      <categories>
        <category>MATLAB</category>
      </categories>
      <tags>
        <tag>MATLAB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MATLAB-实践]]></title>
    <url>%2F2018%2F01%2F02%2FMATLAB%2FMATLAB-%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[Fibonacci 数 产生前 n 个斐波那契数 ### 用递归实现求第 n 个斐波那契数 ### 获取指定位置的 fibonacci 数 计算黄金分割比 使用 for 语句计算 使用 while 语句计算 方程 求积分 有函数 f(x)=x2,画出该函数的曲线（使用 M 函数 ezplot） 修改显示的坐标轴的 X 区间为 0-6 axis([0 6 0 40]) 在 X 坐标 2 和 5 上画到曲线的两条垂直线，颜色为红色，虚线 计算曲线与两条红线围起来的面积（使用 M 函数 int） 解方程 x2-x-1=0 的三种方法 ### 线性方程组求解 可转化为矩阵形式，即为： AX = B A为系数矩阵，B为右边值矩阵 利用矩阵除法：X = A\B 得值 #### 二元线性方程 2x-y=0 -x+2y=3 #### 三元线性方程 3x1+12x2+ x3=2.36 12x1+2x3=5.26 2x2+3x3=2.77 如何判断线性方程组是否有唯一解 通过行列式函数det( )判断 ### 当无唯一解时如何判断无穷多解或无解 通过求rank( ) * 确定是奇异矩阵 * 无穷解：rank(A)==rank(A 的增广矩阵) A 的增广矩阵=[A,b],或 A(:,3)=b * 无解：rank(A)≠rank(A 的增广矩阵) 编写判断线性方程组唯一解、无穷多解和无解的函数 mark 图形绘制 勾 3 股 4 弦 5的直角三角形 ### 绘制 y=x 与 y=sqrt(x+1)两条线，并标识交叉点 ### 绘制黄金矩阵 ### 绘制正弦、余弦图形 #### 用 plot 函数绘制正弦、余弦图形 #### 用 explot 函数绘制正弦、余弦图形 ### 用dot2dot画图 #### 两个点连成的直线 123y = [-5 5 0 0] dot2dot(y) 画一个直角三角形 123y = [ 0 5 0 0 0 8] dot2dot(y) 画 house 12x =[-6 -6 -7 0 7 6 6 -3 -3 0 0 -7 2 1 8 1 2 -7 -7 -2 -2 -7] 蕨型叶分形 旋转蕨型叶图 互换x，y区间 axis([-3 3 0 10 ]) 改为 axis([0 10 -3 3]) 互换 x 和 y 坐标来旋转蕨型叶图 plot(x(1),x(2),'.','markersize',4,'color',pink) gaiwe plot(x(2),x(1),'.','markersize',4,'color',pink) 矩阵 对角矩阵 12A =[1 -1; 0 1][V, D]=eig(A) 旋转 mark 日期 求时间 求生日是在星期几 1[index,d] = weekday(datenum([2017 05 20])) 查询今天是星期几命令 datestr(fix(now),'dddd') 查询三周后是几号命令 datestr(fix(now)+21) 查询当前是几秒（精确有毫秒） datestr(now,'mmmm dd,yyyy HH:MM:SS.FFF AM') ### 判断闰年 ### 黑色星期五 ### 生物韵律 ### 复活节]]></content>
      <categories>
        <category>MATLAB</category>
      </categories>
      <tags>
        <tag>MATLAB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MATLAB]]></title>
    <url>%2F2018%2F01%2F02%2FMATLAB%2FMATLAB-%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[基本概念 - 在结尾加; 结果不显示 - 黄金分割比： - clc清屏 - clear清空数据 - clf清除图表 - a=[ ; ; ] 生成数列 行与行之间用 ; 隔开 - pathtool指定默认路径 - syms 是定义符号变量 - ans会保存未初始化的结果 查 ans（:,:,1） 冒号表示任意列 A=[ 1 2 3 4 5 6 7 8 9] 读取第 2 列 A( :, 2) 读取[8 9] A(3,[2:3]) 读取[5 6 8 9] A([2:3],[2:3]) 读写画 读 读图片 12h=imread('a.jpg');image(h) 读文件 xlsread(文件名,表n,sheet) xlswrite(文件名,矩阵名) cell2mat() 转置数据 sort() 排序 ### 写 #### 写入图形 1saveas(gcf,'test.jpg') 画图 plot golden_spiral 黄金螺旋 - edit golden_spiral 编辑文件 - format long 设置类型 - plot（x,y,－）画图工具 123x = 0:pi/100:2*pi;y = sin(x);plot(x,y) r是换颜色 hold on 保存图表 hold off 取消保存 绘图函数 dot2dot dot2dot(y) 设置 axis off 关闭坐标轴 axis(x,y) 限制x,y的范围 axis equal 让x轴和y轴坐标距离相等 xlabel() 显示x轴的标签 title() 显示标题 legend() 显示图列 图形对象gcf get( gcf) 获取对象的属性 set(gcf,color,red) 设置属性的属性 ezplot跟plot的区别 plot绘制图形时要指定自变量的范围，而ezplot无需数据准备，直接绘出图形。 123sys xy = x^2－x－1ezplot(y) disp() 输出 日期与时钟 日期时间的三种表示及相互转换 日期数字(num)、日期字符串(str)、日期向量(vec) now date lock 转成“日期数字”(双精度) datenum() 转成“日期字符串” datestr() 转为字符串 转成“日期向量” datevec() 其它常用的日期函数 1.[d w]=weakdate(日期数字或字符串) 2.eomday(年，月) %返回一个月的最后一天 3.calendar(年,月) %任何一个月的月历 4.tic命令或程序tok%tick-tock 嘀哒声。 Tic 启动一个秒表，toc 停止这个秒表并计算出所经历的时间。 5.cputime %MATLAB 启动之后所占用的 CPU 时间。 方程 三种求方程的办法 roots 是多项式根 此 MATLAB 函数 以列矢量的形式返回 p 表示的多项式的根。输入 p 是一个包含 n+1 多项式系数的矢量，以 xn 系数开头。0 系数表示方程中不存在的中间幂。例如：p = [3 2 -2]代表多项式3x^2+2x-2 所以x1 = roots([1 -1 -1]) 等于 x1 = x^2-x-1 fsolve是采用最小二乘法来求解非线性方程。 它的一般求解方式为： X=fsolve(fun,X0,options) 其中，fun是要求解的非线性方程，X0是变量初值，options由optimset函数产生的结构体，用于对优化参数的设置，可以省略（采用默认值）。 所以 f = @(x)-sqrt(1+x),p = @(x)x^2-x-1 Fslove(f,1) 表示初始变量x=1，时f()方程的解 #### syms syms 是定义符号变量,solve()内可直接输入函数(在matable中使用) 12solve('x^2-x-1')solve('x-sqrt(1+x)=0') 矩阵 矩阵的同行元素之间用空格或逗号 行与行之间用分号;或回车 #### zeros（i） i阶矩阵 - zeros（3, 4, 5）生成全零矩阵 #### ones( ) 全1矩阵 #### eye( ) 生成单位矩阵 生成对角矩阵 #### magic() #### rand() #### [K,J]=ndgrid( : )]]></content>
      <categories>
        <category>MATLAB</category>
      </categories>
      <tags>
        <tag>MATLAB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实战六（K-均值聚类算法）]]></title>
    <url>%2F2017%2F12%2F27%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E5%85%AD%EF%BC%88K-%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一、概述 1. 原理： 工作机制：给定测试样本，基于某种距离度量找出训练集中的与其最靠近的k个训练样本，然后基于这k个“邻居”的信息来进行预测. 通常k是不大于20的整数 2. 优缺点 优点：精度高、对异常值不敏感、无数据输入假定 缺点：计算复杂度高、空间复杂度高，占用存储空间。无法给出数据的内在含义 适用数据范围：数值型和标称型 3. 懒惰算法： 此类学习技术在训练阶段仅仅把样本保存起来 训练开支为零，待收到测试样本后再进行处理 二、算法流程 收集数据 准备数据 分析数据 训练算法（不适合KNN算法，因为K是“懒惰算法”的著名代表） 测试算法 使用算法 三、算法框架 1. classify0()函数的4个输入参数： inX：用于分类的输入向量 dataSet：用于输入的训练样本集 labels：标签向量 k：参数k用于选择最佳邻居的数目 4. 数据的归一化 由于每个属性的取值数量级相差过大，会造成每个属性的权重不同，这显然是海伦不希望的。所以我们还要写一个函数实现数据归一化，公式如下: newValue=(oldValue-min)/(max-min) 这个公式可以将特征值转化为0~1之间的值。 1234567891011121314#归一化特征值def autoNorm(dataSet): #每列的最小值 minVals=dataSet.min(0) #每列的最大值 maxVals=dataSet.max(0) #最大值与最小值的差值 ranges=maxVals-minVals normDataSet=zeros(shape(dataSet)) m=dataSet.shape[0] #minVals是1*3的矩阵，使用tile函数复制成和dataSet同样大小的矩阵，方便计算 normDataSet=dataSet-tile(minVals,(m,1)) normDataSet=normDataSet/tile(ranges,(m,1)) return normDataSet,ranges,minVals 12&gt;&gt;&gt;reload(kNN) &gt;&gt;&gt;normMat,ranges,minVals=kNN.autoNorm(datingDataMat) 5. 原始分类器 1234567891011121314#原始测试分类器def datingClassTest(): hoRatio=0.10 datingDataMat,datingLabels=file2matrix('datingTestSet2.txt') normMat,ranges,minVals=autoNorm(datingDataMat) m=normMat.shape(0) #10%的数据用于测试数据集 numTestVecs=int(m*hoRatio) errorCount=0.0 for i in range(numTestVecs): classifierResults=classify0(normMat[i,:],normMat[numTestVecs:m,:], datingLabels[numTestVecs:m],3) print("the classifier came back with: %d,the real answer id: %d"%(classifierResults,datingLabels[i])) if(classifierResults!=datingLabels[i]):errorCount +=1.0 print("the total error rate is: %f" %(errorCount/float(numTestVecs))) 6. 接口 1234567891011# 约会网站预测函数def classifyPerson(): resultList=['not at all','in small doses','in large doses'] percentTats=float(input("在游戏上花费的时间占比( )?")) ffMiles=float(input("每年航空的里程数?")) iceCream=float(input("每年吃的冰淇淋（升）?")) datingDataMat,datingLabels=file2matrix('datingTestSet2.txt') normMat, ranges, minVals=autoNorm(datingDataMat) inArr=array([ffMiles,percentTats,iceCream]) classifierResult=classify0((inArr-minVals)/ranges,normMat,datingLabels,3) print("你可能是属于以下这类人：",resultList[classifierResult - 1]) 7. 识别手写数字 1234567891011# 将图片格式处理为一个向量（把一个32*32的二进制图形矩阵转换为1*1024的向量）,然后使用分类器处理图形信息def img2vector(filename): # 创建一个1*1024的NumPy数组 returnVect = zeros((1,1024)) fr = open(filename) # 循环读出文件的前32行，并将每行的头32个字符值存储在NumPy数组中，最后返回数组 for i in range(32): lineStr = fr.readline() for j in range(32): returnVect[0,32*i+j] = int(lineStr[j]) return returnVect 123456789101112131415161718192021222324252627282930313233def handwritingClassTest(): hwLabels = [] # 将trainingDigits目录中的文件存储在列表中 trainingFileList = listdir('trainingDigits') # 获得文件数赋值给m m = len(trainingFileList) # 创建一个m行1024列的训练矩阵，每行数据存储一个图像 trainingMat = zeros((m,1024)) for i in range(m): # 从文件名解析分类数字（eg：文件9_45.txt表示数字9的第45个实例） fileNameStr = trainingFileList[i] fileStr = fileNameStr.split('.')[0] #take off .txt classNumStr = int(fileStr.split('_')[0]) # 将类代码存储在 hwLabels向量中 hwLabels.append(classNumStr) # 使用 img2vector函数载入图像 trainingMat[i,:] = img2vector('trainingDigits/%s' % fileNameStr) # 将testDigits目录中的文件存储在列表中，步骤同上 testFileList = listdir('testDigits') #iterate through the test set errorCount = 0.0 mTest = len(testFileList) for i in range(mTest): fileNameStr = testFileList[i] fileStr = fileNameStr.split('.')[0] #take off .txt classNumStr = int(fileStr.split('_')[0]) vectorUnderTest = img2vector('testDigits/%s' % fileNameStr) # 不同的是要使用 classify0函数测试目录下的每个文件（文件中的值介于0-1之间，无需用autoNorm()函数 classifierResult = classify0(vectorUnderTest, trainingMat, hwLabels, 3) print("the classifier came back with: %d, the real answer is: %d" % (classifierResult, classNumStr)) if (classifierResult != classNumStr): errorCount += 1.0 print("\nthe total number of errors is: %d" % errorCount) print("\nthe total error rate is: %f" % (errorCount/float(mTest))) github代码]]></content>
      <categories>
        <category>机器学习实战</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法概论（聚类）]]></title>
    <url>%2F2017%2F12%2F27%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%2F%E7%AE%97%E6%B3%95%E6%A6%82%E8%AE%BA%EF%BC%88%E8%81%9A%E7%B1%BB%EF%BC%89%2F</url>
    <content type="text"><![CDATA[聚类分析基础 概述 概念 聚类是将数据对象的集合分成相似的对象类的过程。使得同一个簇（或类）中的对象之间具有较高的相似性，而不同簇中的对象具有较高的相异性。 #### 表示形式 聚类分析(clustering analysis)就是根据某种相似性度量标准，将一个没有类别标号的数据集S直接拆分成若干个子集Ci (i=1,2, …,k; m≤n)，并使每个子集内部数据对象之间相似度很高，而不同子集的对象之间不相似或相似度很低。 每个子集Ci称为一个簇，所有簇构成的集合C={C1 ,C2, …,Ck}称为数据集S的一个聚类。 ### 数学定义 称C为S由s(X,Y)生成的一个划分聚类(partitional clustering)，简称C为S的划分聚类，或称为互斥(exclusive)的聚类。 簇的形状 类球状的簇 一般是聚类算法使用距离函数所产生的簇 #### 非球状的簇 非球状的簇，通常由基于密度或基于原型的聚类算法获得的簇。 簇间关系 明显分离的簇 簇中每个数据对象到同簇中其它对象的 距离，比到不同簇中任意对象的距离更近; 显分离的簇不必是球形的，也可以是其它任意的形状。 #### 基于原型的簇 所谓原型其实就是簇中最具代表性的点。对连续属性的数据，簇的原型通常就是质心，即簇中所有点的平均值。 当数据包括分类属性时，簇的原型通常是中心点，即簇中最有代表性的点。 对于许多数据类型，原型可以视为最 靠近中心的点，因此，通常把基于原型的 簇看作基于中心的簇(center-based cluster), 右图就是一个基于中心的簇的例子。 #### 基于连片的簇(contiguity-based cluster) 簇中两个相邻对象的距离都在指定 的阈值之内即将其归为同一个簇。 当簇的形状不规则或缠绕，且数据 集没有噪声时，用这种方式来定义簇会 收到很好的聚类效果。如果存在噪声，其聚类效果就不一定理想。图中的哑铃状簇，就是由线状 (噪声)连接两个球状簇形成的一个簇。 #### 基于密度的簇(density-based cluster) 基于密度的簇由对象间相对稠密的区域组成，且其周围是低密度的区域。一般通过指定簇中任何一个对象周围区域内最少点数(即密度)来实现。 #### 基于概念的簇 即具有某种“共同性质”的数 据对象集合。比如，离相同的质 心或中心点最近，或组成三角形、 梯形，或圆环状(重叠聚类)等。 ### 簇的距离 数据集S的一个聚类C={C1,C2,…,Ck}的质量，包括每个簇Ci的质量和C的总体质量。前者用簇内距离来刻画，后者用簇间距离来衡量。 #### 簇内距离 #### 簇间距离 相似性测度公式 距离相似性度量 通常相似度与距离成反比，在确定好距离函数后，可设计相似度函数如下： ### 密度相似性度量 ### 连通性相似性度量 数据集用图表示，图中结点是对象，而边代表对象之间的联系，这种情况下可以使用连通性相似性，将簇定义为图的连通分支，即图中互相连通但不与组外对象连通的对象组。 也就是说，在同一连通分支中的对象之间的相似性度量大于不同连通分支之间对象的相似性度量。 ### 概念相似性度量 若聚类方法是基于对象具有的概念，则需要采用概念相似性度量，共同性质（比如最近邻）越多的对象越相似。簇定义为有某种共同性质的对象的集合。 ## k-means ### 算法描述 k-means算法也称k-平均算法，它采用距离作为相异度的评价指标，以簇内差异函数w(C)作为聚类质量的优化目标函数，即将所有数据对象到它的簇中心点的距离平方和作为目标函数，算法寻找最优聚类的策略是使目标函数达到最小值(簇中心不变化等价于w(C)达最小)。 ### 算法的优缺点 #### 算法的优点 * k-平均算法简单、经典，常作为其它聚类算法的参照或被改进。 * k-平均算法以k个簇的误差平方和最小为目标，当聚类的每个簇是密集的，且簇与簇之间区别明显时，其聚类效果较好。 * k-平均算法处理大数据集高效，且具较好的可伸缩性，其计算复杂性为O(nkt)，n是数据对象个数，k为簇个数，t是迭代次数。 (2) 算法的缺点 * k-平均算法对初始中心点的选择比较敏感。对同一个数据集，如果初始中心选择不同，其聚类结果也可能不一样。 * k-平均算法对参数k是比较敏感的，即使是同一个数据集，如果k选择不同，其聚类结果可能完全不一样。 * k-平均算法以簇内对象的平均值作为簇中心来计算簇内误差，在连续属性的数据集上很容易实现，但在具有离散属性的数据集上却不能适用。 离群点(Outlier-异常点-噪音) 离群点的产生 数据来源于欺诈、入侵、疾病爆发、不寻常的实验结果等引起的异常。 比如，某人话费平均200元左右，某月突然增加到数千元；某人的信用卡通常每月消费5000元左右，而某个月消费超过3万等。 这类离群点在数据挖掘中通常都是相对有趣的，应用的重点之一。 #### 数据变量固有变化引起，反映了数据分布的自然特征 如气候变化、顾客新的购买模式、基因突变等。也是有趣的重点领域之一。 #### 数据测量和收集误差，主要是由于人为错误、测量设备故障或存在噪音。 例如，一个学生某门课程的成绩为100，可能是由于程序设置默认值引起的；一个公司的高层管理人员的工资明显高于普通员工的工资看上去像是一个离群点，但却是合理的数据。 ### 离群点挖掘问题 #### 定义离群点 由于离群点与实际问题密切相关，明确定义什么样的数据是离群点或异常数据，是离群点挖掘的前提和首要任务，一般需要结合领域专家的经验知识，才能对离群点给出恰当的描述或定义。 #### 挖掘离群点 离群点被明确定义之后，用什么算法有效地识别或挖掘出所定义的离群点则是离群点挖掘的关键任务。离群点挖掘算法通常从数据能够体现的规律角度为用户提供可疑的离群点数据，以便引起用户的注意。 #### 理解离群点 对挖掘结果的合理解释、理解并指导实际应用是离群点挖掘的目标。由于离群点产生的机制是不确定的，离群点挖掘算法检测出来的“离群点”是否真正对应实际的异常行为，不可能由离群点挖掘算法来说明和解释，而只能由行业或领域专家来理解和解释说明。 离群点的相对性 全局或局部的离群点 一个数据对象相对于它的局部近邻对象可能是离群的，但相对于整个数据集却不是离群的。如身高1.9米的同学在我校数学专业1班就是一个离群点，但在包括姚明等职业球员在内的全国人民中就不是了。 #### 离群点的数量 离群点的数量虽是未知的，但正常点的数量应该远远超过离群点的数量，即离群点的数量在大数据集中所占的比例应该是较低的，一般认为离群点数应该低于5%甚至低于1%。 #### 点的离群因子 不能用“是”与“否”来报告对象是否为离群点，而应以对象的偏离程度，即离群因子(Outlier Factor)或离群值得分(Outlier Score)来刻画一个数据偏离群体的程度，然后将离群因子高于某个阈值的对象过滤出来，提供给决策者或领域专家理解和解释，并在实际工作中应用。 判断离群点的方法 基于距离的方法 基于相对密度的方法 聚类算法的评价 一个好的聚类算法产生高质量的簇，即高的簇内相似度和低的簇间相似度。 ### 内部质量评价准则 ### 外部质量评价准则]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试大纲]]></title>
    <url>%2F2017%2F12%2F24%2F%E5%91%A8%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[请你自我介绍一下你自己， 回答提示：xxx、大三，我从大二开始自学python，对python有一定的了解。大二下学期开始对机器学习感兴趣，：最强的技能、最深入研究的知识领域、个性中最积极的部分、做过的最成功的事，主要的成就等，这些都可以和学习无关，也可以和学习有关，但要突出积极的个性和做事的能力，说得合情合理企业才会相信。企业很重视一个人的礼貌，求职者要尊重考官，在回答每个问题之后都说一句“谢谢”。企业喜欢有礼貌的求职者。 你觉得你个性上最大的优点是什么？ 回答提示：沉着冷静、条理清楚、立场坚定、细心。 自学能力强、适应能力。 3. 说说你最大的缺点？ 回答提示：我觉得我在对一件还未完成的事情会有急迫感。 4. 你对加班的看法？ 回答提示：实际上好多公司问这个问题，并不证明一定要加班。 只是想测试你是否愿意为公司奉献。 回答样本：如果是工作需要的话加班是正常的。但同时，我觉得工作效率比加班更重要。我更希望能提高工作效率，减少不必要的加班。 5. 你对薪资的要求？ 回答提示：如果你对薪酬的要求太低，那显然贬低自己的能力；如果你对薪酬的要求太高，那又会显得你分量过重，公司受用不起。一些雇主通常都事先对求聘的职位定下开支预算，因而他们第一次提出的价钱往往是他们所能给予的最高价钱。他们问你只不过想证实一下这笔钱是否足以引起你对该工作的兴趣。 回答样本一：我之前在网上也查了下贵公司的一些情况，跟基本工资情况。但“我对工资没有硬性要求。我相信贵公司在处理我的问题上会友善合理。我注重的是找对工作机会，所以只要条件公平，我则不会计较太多 6. 在五年的时间内，你的职业规划？ 回答提示：我准备在技术领域：比如数据挖掘、机器学习的大方向上进行提高 7. 你朋友对你的评价？ 回答提示： 想从侧面了解一下你的性格及与人相处的问题。 回答样本：“我的朋友都说我是一个可以信赖的人。因为，我一旦答应别人的事情，就一定会做到。如果我觉得我做不到，我会立刻提出问题所在，希望止损最少。 8. 你还有什么问题要问吗？ 回答样本：贵公司有对新入公司的员工的培训项目吗？或者说贵公司的晋升机制是什么样的？企业将很欢迎，因为体现出你对学习的热情和对公司的忠诚度以及你的上进心。 9. 如果通过这次面试我们单位录用了你，但工作一段时间却发现你根本不适合这个职位，你怎么办？ 回答：就我本人而言，我来面试就说明我会对贵公司及该岗位有一定的理解与兴趣 10. 在完成某项工作时，你认为领导要求的方式不是最好的，自己还有更好的方法，你应该怎么做？ 回答提示： 原则上我会尊重和服从领导的工作安排；同时私底下找机会以请教的口吻，婉转地表达自己的想法，看看领导是否能改变想法； 11. 如果你的工作出现失误，给本公司造成经济损失，你认为该怎么办？ 回答提示： ①弥补或挽回经济损失 ②分清责任，各负其责，如果是我的责任，我甘愿受罚；如果是一个我负责的团队中别人的失误，也不能幸灾乐祸，作为一个团队，需要互相提携共同完成工作，安慰同事并且帮助同事查找原因总结经验。 ③总结经验教训，一个人的一生不可能不犯错误，重要的是能从自己的或者是别人的错误中吸取经验教训，并在今后的工作中避免发生同类的错误。检讨自己的工作方法、分析问题的深度和力度是否不够，以致出现了本可以避免的错误。 12. 如果你在这次考试中没有被录用，你怎么打算？ 回答提示： 第二、善于反思,对于这次面试经验要认真总结,思考剖析,能够从自身的角度找差距。正确对待自己,实事求是地评价自己,辩证的看待自己的长短得失,做一个明白人. 第四、认真工作,回到原单位岗位上后,要实实在在、踏踏实实地工作,三十六行,行行出状元,争取在本岗位上做出一定的成绩. 14. 谈谈你对跳槽的看法？ 回答提示：（1）正常的&quot;跳槽&quot;能促进人才合理流动，应该支持； （2）频繁的跳槽对单位和个人双方都不利，应该反对。 15. 工作中你难以和同事、上司相处，你该怎么办？ 回答提示： ①我会服从领导的指挥，配合同事的工作。 ② 我会从自身找原因，仔细分析是不是自己工作做得不好让领导不满意，同事看不惯。还要看看是不是为人处世方面做得不好。如果是这样的话 我会努力改正。 ③ 如果我找不到原因，我会找机会跟他们沟通，请他们指出我的不足。有问题就及时改正。 ④作为优秀的员工，应该时刻以大局为重，即使在一段时间内，领导和同事对我不理解，我也会做好本职工作，虚心向他们学习，我相信，他们会看见我在努力，总有一天会对我微笑的！ 18. 你对于我们公司了解多少？ 回答提示：在去公司面试前上网查一下该公司主营业务。如回答：贵公司有意改变策略，加强与国外大厂的OEM合作，自有品牌的部分则透过海外经销商。 19. 请说出你选择这份工作的动机？ 回答提示：这是想知道面试者对这份工作的热忱及理解度，并筛选因一时兴起而来应试的人 20. 你最擅长的技术方向是什么？ 回答提示：说和你要应聘的职位相关的课程，表现一下自己的热诚没有什么坏处。 21. 你能为我们公司带来什么呢？ 回答提示： ①我对该职位需要掌握的技能较为熟悉与了解，能较好的学习与使用新的技术为公司创造收益 ②企业很想知道未来的员工能为企业做什么，求职者应再次重复自己的优势，然后说：“就我的能力，我可以做一个优秀的员工在组织中发挥能力，给组织带来高效率和更多的收益”。企业喜欢求职者就申请的职位表明自己的能力， 22. 最能概括你自己的三个词是什么？ 回答提示： 我经常用的三个词是：适应能力强，有责任心和做事有始终，结合具体例子向主考官解释， 23. 你的业余爱好是什么？ 回答提示：找一些富于团体合作精神的。主考官说：因为这是一项单人活动，我不敢肯定他能否适应团体工作。 24. 作为被面试者给我打一下分 回答提示：试着列出四个优点和一个非常非常非常小的缺点，（可以抱怨一下设施，没有明确责任人的缺点是不会有人介意的）。 25. 你怎么理解你应聘的职位？ 回答提示：把岗位职责和任务及工作态度阐述一下 26. 喜欢这份工作的哪一点？ 回答提示：相信其实大家心中一定都有答案了吧！每个人的价值观不同，自然评断的标准也会不同，但是，在回答面试官这个问题时可不能太直接就把自己心理的话说出来，尤其是薪资方面的问题，不过一些无伤大雅的回答是不错的考虑，如交通方便，工作性质及内容颇能符合自己的兴趣等等都是不错的答案，不过如果这时自己能仔细思考出这份工作的与众不同之处，相信在面试上会大大加分。 28. 说说你对行业、技术发展趋势的看法？ 回答提示：企业对这个问题很感兴趣，只有有备而来的求职者能够过关。求职者可以直接在网上查找对你所申请的行业部门的信息，只有深入了解才能产生独特的见解。企业认为最聪明的求职者是对所面试的公司预先了解很多，包括公司各个部门，发展情况，在面试回答问题的时候可以提到所了解的情况，企业欢迎进入企业的人是“知己”，而不是“盲人”。 29. 对工作的期望与目标何在？ 回答提示：这是面试者用来评断求职者是否对自己有一定程度的期望、对这份工作是否了解的问题。对于工作有确实学习目标的人通常学习较快，对于新工作自然较容易进入状况，这时建议你，最好针对工作的性质找出一个确实的答案，如业务员的工作可以这样回答：“我的目标是能成为一个超级业务员，将公司的产品广泛的推销出去，达到最好的业绩成效；为了达到这个目标，我一定会努力学习，而我相信以我认真负责的态度，一定可以达到这个目标。”其他类的工作也可以比照这个方式来回答，只要在目标方面稍微修改一下就可以了。 31. 就你申请的这个职位，你认为你还欠缺什么？ 回答提示：继续重复自己的优势，然后说：“对于这个职位和我的能力来说，我相信自己是可以胜任的，只是缺乏经验，这个问题我想我可以进入公司以后以最短的时间来解决，我的学习能力很强，我相信可以很快融入公司的企业文化，进入工作状态。”企业喜欢能够巧妙地躲过难题的求职者。 32. 你欣赏哪种性格的人？ 回答提示：诚实、不死板而且容易相处的人、有&quot;实际行动&quot;的人。 33. 你通常如何处理別人的批评？ 回答提示：①沈默是金。不必说什么，否则情况更糟，不过我会接受建设性的批评；②我会等大家冷靜下来再讨论。 34. 你怎样对待自己的失敗？ 回答提示：我们大家生来都不是十全十美的，我相信我有第二个机会改正我的错误。 39. 问题：你做过的哪件事最令自己感到骄傲? 回答提示：这是考官给你的一个机会，让你展示自己把握命运的能力。这会体现你潜在的领导能力以及你被提升的可能性。假如你应聘于一个服务性质的单位，你很可能会被邀请去午餐。记住：你的前途取决于你的知识、你的社交能力和综合表现。 41. 对这项工作，你有哪些可预见的困难？” 回答提示：可以尝试迂回战术，说出应聘者对困难所持有的态度——“工作中出现一些困难是正常的，也是难免的，但是只要有坚忍不拔的毅力、良好的合作精神以及事前周密而充分的准备，任何困难都是可以克服。” 分析：一般问这个问题，面试者的希望就比较大了，因为已经在谈工作细节。但常规思路中的回答，又被面试官“骗”了。当面试官询问这个问题的时候，有两个目的。第一，看看应聘者是不是在行，说出的困难是不是在这个职位中一般都不可避免的问题。第二，是想看一下应聘者解决困难的手法对不对，及公司能否提供这样的资源。而不是想了解应聘者对困难的态度。 42. 如果我录用你，你将怎样开展工作？” 回答提示： ①如果应聘者对于应聘的职位缺乏足够的了解，最好不要直接说出自己开展工作的具体办法；②可以尝试采用迂回战术来回答，如“首先听取领导的指示和要求，然后就有关情况进行了解和熟悉，接下来制定一份近期的工作计划并报领导批准，最后根据计划开展工作。” 分析：这个问题的主要目的也是了解应聘者的工作能力和计划性、条理性，而且重点想要知道细节。如果向思路中所讲的迂回战术，面试官会认为回避问题，如果引导了几次仍然是回避的话。此人绝对不会录用了。 43. “你希望与什么样的上级共事？” 分析：希望我的上级能够在工作中对我多指导，对我工作中的错误能够立即指出。总之，从上级指导这个方面谈，不会有大的纰漏。 44. 在完成某项工作时，你认为领导要求的方式不是最好的，自己还有更好的方法，你应该怎么做？ 回答提示： ①.原则上我会尊重和服从领导的工作安排；同时私底下找机会以请教的口吻，婉转地表达自己的想法，看看领导是否能改变想法； ②如果领导没有采纳我的建议，我也同样会按领导的要求认真地去完成这项工作； ③.还有一种情况，假如领导要求的方式违背原则，我会坚决提出反对意见；如领导仍固执己见，我会毫不犹豫地再向上级领导反映。 46. “你工作经验欠缺，如何能胜任这项工作？” 常规思路：①如果招聘单位对应届毕业生的应聘者提出这个问题，说明招聘公司并不真正在乎“经验”，关键看应聘者怎样回答；②对这个问题的回答最好要体现出应聘者的诚恳、机智、果敢及敬业；③如“作为应届毕业生，在工作经验方面的确会有所欠缺，因此在读书期间我一直利用各种机会在这个行业里做兼职。我也发现，实际工作远比书本知识丰富、复杂。但我有较强的责任心、适应能力和学习能力，而且比较勤奋，所以在兼职中均能圆满完成各项工作，从中获取的经验也令我受益非浅。请贵公司放心，学校所学及兼职的工作经验使我一定能胜任这个职位。” 点评：这个问题思路中的答案尚可。突出自己的吃苦能力和适应性以及学习能力（不是学习成绩）为好。 51. 假如你晚上要去送一个出国的同学去机场，可单位临时有事非你办不可，你怎么办？ 回答提示：我觉得工作是第一位的，但朋友间的情谊也是不能偏废的。这个问题我觉得要按照当时具体的情况来决定。 （1）、如果我的朋友晚上9点中的飞机，而我的 加班八点就能够完成的话，那就最理想了，干完工作去机场，皆大欢喜。 （2）、如果说工作不是很紧急，加班仅仅是为了明天上班的时候能把报告交到办公室，那完全可以跟领导打声招呼，先去机场然后回来加班，晚点睡就是了。 （3）、如果工作很紧急，两者不可能兼顾的情况下，我觉得可以由两种选择。1）如果不是全单位都加班的话，是不是可以要其他同事来代替以下工作，自己去机场，哪怕就是代替你离开的那一会儿。2）如果连这一点都做不到的话， 那只好忠义不能两全了，打电话给朋友解释一下，小心他会理解，毕竟工作做完了就完了，朋友还是可以再见面的。 58. 对这个职务的期许？ 回答提示：希望能借此发挥我的所学及专长，同时也吸收贵公司在这方面的经验，就公司、我个人而言，缔造“双赢”的局面。 分析：回答前不妨先询问该公司对这项职务的责任认定及归属，因为每一家公司的状况不尽相同。以免说了一堆理想抱负却发现牛头不对马嘴。 59. 为什么选择这个职务？ 回答提示：：这一直是我的兴趣和专长，经过这几年的磨练，也累积了一定的经验及人脉，相信我一定能胜任这个职务的。 分析：适时举出过去的“丰功伟业”，表现出你对这份职务的熟稔度，但避免过于夸张的形容或流于炫耀。 62. 请谈谈如何适应办公室工作的新环境？ 回答提示①办公室里每个人有各自的岗位与职责，不得擅离岗位。 ②根据领导指示和工作安排，制定工作计划，提前预备，并按计划完成。 ③多请示并及时汇报，遇到不明白的要虚心请教。 ④抓间隙时间，多学习，努力提高自己的政治素质和业务水平。 65. 最能概括你自己的三个词是什么？ 回答提示：我经常用的三个词是：适应能力强，有责任心和做事有始终，结合具体例子向主考官解释，使他们觉得你具有发展潜力 67. 除了本公司外，还应聘了哪些公司？ 回答提示：很奇怪，这是相当多公司会问的问题，其用意是要概略知道应徵者的求职志向，所以这并非绝对是负面答案，就算不便说出公司名称，也应回答“销售同种产品的公司”，如果应聘的其他公司是不同业界，容易让人产生无法信任的感觉。 68. 何时可以到职？ 回答提示：大多数企业会关心就职时间，最好是回答’如果被录用的话，到职日可按公司规定上班”，但如果还未辞去上一个工作、上班时间又太近，似乎有些强人所难，因为交接至少要一个月的时间，应进一步说明原因，录取公司应该会通融的]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow基础]]></title>
    <url>%2F2017%2F12%2F23%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BA%93%2FTensorFlow%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[例子1 创建数据 加载 tensorflow 和 numpy 两个模块, 并且使用 numpy 来创建我们的数据. 123456import tensorflow as tfimport numpy as np# create datax_data = np.random.rand(100).astype(np.float32)y_data = x_data*0.1 + 0.3 搭建模型 用 tf.Variable 来创建描述 y 的参数. 我们可以把y_data = x_data*0.1 + 0.3想象成y=Weights * x + biases, 然后神经网络也就是学着把 Weights 变成 0.1, biases 变成 0.3. 1234Weights = tf.Variable(tf.random_uniform([1], -1.0, 1.0))biases = tf.Variable(tf.zeros([1]))y = Weights*x_data + biases 计算误差 计算 y 和 y_data 的误差 1loss = tf.reduce_mean(tf.square(y-y_data)) 传播误差 用optimizer实现反向传递误差的工作 使用的误差传递方法是梯度下降法: Gradient Descent 让后我们使用 optimizer 来进行参数的更新. 12optimizer = tf.train.GradientDescentOptimizer(0.5)train = optimizer.minimize(loss) 训练 到目前为止, 我们只是建立了神经网络的结构, 还没有使用这个结构. 在使用这个结构之前, 我们必须先初始化所有之前定义的Variable, 所以这一步是很重要的! 12# init = tf.initialize_all_variables() # tf 马上就要废弃这种写法init = tf.global_variables_initializer() # 替换成这样就好 创建会话 Session 来执行 init 初始化步骤。并且, 用 Session 来 run 每一次 training 的数据. 逐步提升神经网络的预测准确性. 1234567sess = tf.Session()sess.run(init) # Very importantfor step in range(201): sess.run(train) if step % 20 == 0: print(step, sess.run(Weights), sess.run(biases)) 基础架构 Session 会话控制 概念 Session 是 Tensorflow 为了控制,和输出文件的执行的语句. 运行 session.run() 可以获得你要得知的运算结果, 或者是你所要运算的部分. #### 简单运用 加载 Tensorflow ，然后建立两个 matrix ,输出两个 matrix 矩阵相乘的结果。 12345678import tensorflow as tf# create two matrixesmatrix1 = tf.constant([[3,3]])matrix2 = tf.constant([[2], [2]])product = tf.matmul(matrix1,matrix2) 因为 product 不是直接计算的步骤, 所以我们会要使用 Session 来激活 product 并得到计算结果. 有两种形式使用会话控制 Session 。 123456789101112# method 1sess = tf.Session()result = sess.run(product)print(result)sess.close()# [[12]]# method 2with tf.Session() as sess: result2 = sess.run(product) print(result2)# [[12]] Variable 变量 概念 在Tensorflow 中，定义了某字符串是变量，它才是变量，这一点是与 Python 所不同的。 #### 定义变量 定义语法： state = tf.Variable() 123456789101112import tensorflow as tfstate = tf.Variable(0, name='counter')# 定义常量 oneone = tf.constant(1)# 定义加法步骤 (注: 此步并没有直接计算)new_value = tf.add(state, one)# 将 State 更新成 new_valueupdate = tf.assign(state, new_value) 初始化变量 注：定义了变量，一定要进行初始化 12# init = tf.initialize_all_variables() # tf 马上就要废弃这种写法init = tf.global_variables_initializer() # 替换成这样就好 到这里变量还是没有被激活，需要再在 sess 里, sess.run(init) , 激活 init 这一步. 123456# 使用 Sessionwith tf.Session() as sess: sess.run(init) for _ in range(3): sess.run(update) print(sess.run(state)) 注：一定要把 sess 的指针指向 state 再进行 print 才能得到想要的结果！ placeholder 概念 placeholder是 Tensorflow 中的占位符，暂时储存变量. Tensorflow 如果想要从外部传入data, 那就需要用到tf.placeholder(), 然后以这种形式传输数据sess.run(***, feed_dict={input: **}). #### 例子 12345678import tensorflow as tf#在 Tensorflow 中需要定义 placeholder 的 type ，一般为 float32 形式input1 = tf.placeholder(tf.float32)input2 = tf.placeholder(tf.float32)# mul = multiply 是将input1和input2 做乘法运算，并输出为 output ouput = tf.multiply(input1, input2) 接下来, 传值的工作交给了sess.run() , 需要传入的值放在了feed_dict={}并一一对应每一个 input、placeholder与 feed_dict={}是绑定在一起出现的。 123with tf.Session() as sess: print(sess.run(ouput, feed_dict=&#123;input1: [7.], input2: [2.]&#125;))# [ 14.] placeholder和variable的区别 placeholder 是你输入自己数据的接口, variable 是网络自身的变量, 通常不是你来进行修改, 而是网络自身会改动更新.]]></content>
      <categories>
        <category>机器学习相关库</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow安装与配置]]></title>
    <url>%2F2017%2F12%2F21%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BA%93%2FTensorFlow%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[什么是TensorFlow? TensorFlow是Google开发的一款神经网络的Python外部的结构包, 也是一个采用数据流图来进行数值计算的开源软件库.TensorFlow 让我们可以先绘制计算结构图, 也可以称是一系列可人机交互的计算操作, 然后把编辑好的Python文件 转换成 更高效的C++, 并在后端进行计算. 安装 Anaconda下安装TensorFlow Windows下我使用Anaconda,基本包括了python机器学习需要的库 结果我的Anaconda下并没有TensorFlow，老老实实的安装 搭建TensorFlow 依赖环境 在 Anaconda Prompt 窗口输入： conda create -n tensorflow python=3.5 按提示安装依赖包 激活tensorflow虚拟环境（这里选择激活） activate tensorflow 关闭tensorflow虚拟环境 1deactivate tensorflow 安装TensorFlow 在TensorFlow环境下安装 CPU 版本，CPU的其他版本参考清华大学开源 1pip install -i https://pypi.tuna.tsinghua.edu.cn/simple/ https://mirrors.tuna.tsinghua.edu.cn/tensorflow/windows/cpu/tensorflow-1.1.0-cp35-cp35m-win_amd64.whl 安装spyder 1conda install spyder 其他请看 测试 开启TensorFlow环境 activate tensorflow 打开spyder 1spyder spyder下 1import TensorFlow 如果没有报错，安装成功]]></content>
      <categories>
        <category>机器学习相关库</category>
      </categories>
      <tags>
        <tag>Anaconda</tag>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn进阶]]></title>
    <url>%2F2017%2F12%2F20%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BA%93%2FSklearn%E8%BF%9B%E9%98%B6%2F</url>
    <content type="text"><![CDATA[正交化 Normalization 数据标准化 preprocessing.scale( ) preprocessing.minmax_scale(X,feature_range=(0,1) ): feature_range是设置数据标准化后数据的范围，默认为0~1 1234567891011121314from sklearn import preprocessing #标准化数据模块import numpy as np#建立Arraya = np.array([[10, 2.7, 3.6], [-100, 5, -2], [120, 20, 40]], dtype=np.float64)# 打印出原来的aprint(a)#将normalized后的a打印出print(preprocessing.scale(a))# [[ 0. -0.85170713 -0.55138018]# [-1.22474487 -0.55187146 -0.852133 ]# [ 1.22474487 1.40357859 1.40351318]] 加载模块 123456789101112131415# 标准化数据模块from sklearn import preprocessing import numpy as np# 将资料分割成train与test的模块from sklearn.model_selection import train_test_split# 生成适合做classification资料的模块from sklearn.datasets.samples_generator import make_classification # Support Vector Machine中的Support Vector Classifierfrom sklearn.svm import SVC # 可视化数据的模块import matplotlib.pyplot as plt 生成数据-生成适合做Classification数据 12345678910#生成具有2种属性的300笔数据X, y = make_classification( n_samples=300, n_features=2, n_redundant=0, n_informative=2, random_state=22, n_clusters_per_class=1, scale=100)#可视化数据plt.scatter(X[:, 0], X[:, 1], c=y)plt.show() ### 训练数据 12345X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)clf = SVC()clf.fit(X_train, y_train)print(clf.score(X_test, y_test))# 0.477777777778 标准化数据及可视化 123456789X = preprocessing.scale(X)#可视化数据，显示标准化后范围plt.scatter(X[:, 0], X[:, 1], c=y)plt.show()X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)clf = SVC()clf.fit(X_train, y_train)print(clf.score(X_test, y_test))# 0.9 标准化后数据]]></content>
      <categories>
        <category>机器学习相关库</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>scikit-learn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Anaconda基础]]></title>
    <url>%2F2017%2F12%2F19%2FAnaconda%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[安装与更新 安装源 在 https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/ 寻找你与你电脑系统对应的版本进行安装 ### 测试 下载并安装完成后，打开Anaconda Prompt， 输入 ‘conda –V’, 如果输出如下信息 conda 4.3.30 说明Anaconda 安装成功。 ### 设置 Anaconda 仓库镜像（提高下载速度） 打开 Anaconda Prompt, 输入： 12conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --set show_channel_urls yes 更新包 输入 ‘conda list’查看安装的包 输入‘conda update package_name’更新单个包 1conda upgrade --all #更新全部包 工作路径设置(jupyter notebook的使用) 方法一 找到Anaconda创建的快捷方式 目标中的属性如下 D:3.exe......D:/ProgramData/Anaconda3/Scripts/jupyter-notebook-script.py&quot; %USERPROFILE% 把最后的%USERPROFILE%改为你要用的目录地址如D:]]></content>
      <categories>
        <category>Anaconda</category>
      </categories>
      <tags>
        <tag>Anaconda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sklearn基础]]></title>
    <url>%2F2017%2F12%2F18%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BA%93%2FSklearn%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[简介与安装 概述 Scikit learn 也简称 sklearn, 是机器学习领域当中最知名的 python 模块之一.其包含了许多机器学习的方法的 Sklearn 把所有机器学习的模式整合统一起来了，学会了一个模式就可以通吃其他不同类型的学习模式。 ### 安装 * Windows下使用Anaconda(https://www.anaconda.com/download/) 选择学习方法（模型流程） 安装完 Sklearn 后，不要直接去用，先了解一下都有什么模型方法，然后选择适当的方法，来达到你的目标。 ### 看流程图选方法 klearn 官网提供了一个流程图，蓝色圆圈内是判断条件，绿色方框内是可以选择的算法： ### 详解 从 START 开始，首先看数据的样本是否 &gt;50，小于则需要收集更多的数据。 由图中，可以看到算法有四类，分类，回归，聚类，降维。 监督学习：有数据可以训练 非监督学习： 无数据训练的过程 其中 分类和回归是监督式学习，即每个数据对应一个 label。 聚类 是非监督式学习，即没有 label。 另外一类是 降维，当数据集有很多很多属性的时候，可以通过 降维 算法把属性归纳起来。 例如 ：20 个属性只变成 2 个，注意，这不是挑出 2 个，而是压缩成为 2 个，它们集合了 20 个属性的所有特征，相当于把重要的信息提取的更好，不重要的信息就不要了。 然后看问题属于哪一类问题，是分类还是回归，还是聚类，就选择相应的算法。 当然还要考虑数据的大小，例如 100K 是一个阈值。 可以发现有些方法是既可以作为分类，也可以作为回归，例如 SGD。 ## 基础应用（以KNN classifiler为例） ### 要点 Sklearn 本身就有很多数据库，可以用来练习。 以 Iris 的数据为例，这种花有四个属性，花瓣的长宽，茎的长宽，根据这些属性把花分为三类。 我们要用 分类器 去把四种类型的花分开。 今天用 KNN classifier，就是选择几个临近点，综合它们做个平均来作为预测值。 导入模块 123from sklearn import datasetsfrom sklearn.model_selection import train_test_splitfrom sklearn.neighbors import KNeighborsClassifier 创建数据 加载 iris 的数据，把属性存在 X，类别标签存在 y： 123iris = datasets.load_iris()iris_X = iris.datairis_y = iris.target 观察一下数据集，X 有四个属性，y 有 0，1，2 三类： 123456789101112print(iris_X[:2, :])print(iris_y)"""[[ 5.1 3.5 1.4 0.2] [ 4.9 3. 1.4 0.2]][0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2] """ 把数据集分为训练集和测试集，其中 test_size=0.3，即测试集占总数据的 30%： 可以看到分开后的数据集，顺序也被打乱，这样更有利于学习模型： 123456789X_train, X_test, y_train, y_test = train_test_split( iris_X, iris_y, test_size=0.3)print(y_train)"""[2 1 0 1 0 0 1 1 1 1 0 0 1 2 1 1 1 0 2 2 1 1 1 1 0 2 2 0 2 2 2 2 2 0 1 2 2 2 2 2 2 0 1 2 2 1 1 1 0 0 1 2 0 1 0 1 0 1 2 2 0 1 2 2 2 1 1 1 1 2 2 2 1 0 1 1 0 0 0 2 0 1 0 0 1 2 0 2 2 0 0 2 2 2 1 2 0 0 2 1 2 0 0 1 2] """ 建立模型－训练－预测 定义模块方式 KNeighborsClassifier()， 用 fit 来训练 training data，这一步就完成了训练的所有步骤， 后面的 knn 就已经是训练好的模型，可以直接用来 predict 测试集的数据， 对比用模型预测的值与真实的值，可以看到大概模拟出了数据，但是有误差，是不会完完全全预测正确的。 1234567891011knn = KNeighborsClassifier()knn.fit(X_train, y_train)print(knn.predict(X_test))print(y_test)"""[2 0 0 1 2 2 0 0 0 1 2 2 1 1 2 1 2 1 0 0 0 2 1 2 0 0 0 0 1 0 2 0 0 2 1 0 1 0 0 1 0 1 2 0 1][2 0 0 1 2 1 0 0 0 1 2 2 1 1 2 1 2 1 0 0 0 2 1 2 0 0 0 0 1 0 2 0 0 2 1 0 1 0 0 1 0 1 2 0 1] """]]></content>
      <categories>
        <category>机器学习相关库</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>scikit-learn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo优化相关]]></title>
    <url>%2F2017%2F12%2F04%2Fhexo%E7%9B%B8%E5%85%B3%2FHexo%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[速度优化 Hexo博客优化，如何加快你的博客访问速度 主题优化 参考next主题优化 next主题外观优化 侧边栏社交小图标设置 实现效果图 具体实现方法 打开主题配置文件（_config.yml），搜索social_icons:,在图标库找自己喜欢的小图标，并将名字复制在如下位置 在网站底部加上访问量 具体实现方法 打开\themes\next\layout_partials\footer.swig文件,在copyright前加上画红线这句话： 代码如下： 1&lt;script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"&gt;&lt;/script&gt; 然后再合适的位置添加显示统计的代码，如图： 代码如下: 12345&lt;div class="powered-by"&gt;&lt;i class="fa fa-user-md"&gt;&lt;/i&gt;&lt;span id="busuanzi_container_site_uv"&gt; 本站访客数:&lt;span id="busuanzi_value_site_uv"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/div&gt; 在这里有两中不同计算方式的统计代码： 1. pv的方式，单个用户连续点击n篇文章，记录n次访问量 123&lt;span id="busuanzi_container_site_pv"&gt; 本站总访问量&lt;span id="busuanzi_value_site_pv"&gt;&lt;/span&gt;次&lt;/span&gt; uv的方式，单个用户连续点击n篇文章，只记录1次访客数 123&lt;span id="busuanzi_container_site_uv"&gt; 本站总访问量&lt;span id="busuanzi_value_site_uv"&gt;&lt;/span&gt;次&lt;/span&gt; 网站底部字数统计 切换到根目录下，然后运行如下代码（这里用cnpm） 1$ cnpm install hexo-wordcount --save 然后在/themes/next/layout/_partials/footer.swig文件尾部加上: 1234&lt;div class="theme-info"&gt; &lt;div class="powered-by"&gt;&lt;/div&gt; &lt;span class="post-count"&gt;博客全站共&#123;&#123; totalcount(site) &#125;&#125;字&lt;/span&gt;&lt;/div&gt; 为博客添加萌萌哒的宠物 国外的作者大神的仓鼠 看版娘(hexo集成了，所以设置比仓鼠简单点)大佬github 步骤一 ：安装hexo-helper-live2d插件 1cnpm install -save hexo-helper-live2d 步骤二 ： 将如下代码加入主题 路径：打开Hexo/blog/themes/next/layout 的_layout.swig,将下面代码放到&lt;/body&gt;之前： 1&#123;&#123; live2d() &#125;&#125; 步骤三 ： 参数配置 在 hexo 的_config.yml中添加参数： 12live2d: model: nipsilon 下面是一些model,可以换不同的宠物 12345678910111213141516171819202122232425model 模型名称 默认值: z16 Gantzert_FelixanderEpsilon2.1harumikuni-jniconitonipsilonnietzscheshizukutsumikiwankoz16hibikikoharuharutoUnitychantororohijikiwidth 宽度 默认值: 150height 高度 默认值： 300className &lt;canvas&gt;元素的类名 默认值： live2did &lt;canvas&gt; 元素的id 默认值： live2dcanvasbottom &lt;canvas&gt; 元素的底部偏移 默认值： -20 如果嫌模型位置不正确 可以调整这个参数 文章优化 文章链接持久化 安装hexo-abbrlink 1npm install hexo-abbrlink --save 站点配置文件中 123456permalink: posts/:abbrlink/ # “posts/” 可自行更换# abbrlink configabbrlink: alg: crc32 #support crc16(default) and crc32 rep: hex #support dec(default) and hex 阅读全文设置 方法一：自动截取显示 用文本编辑器打开next目录下的_config.yml文件 12345# Automatically Excerpt. Not recommend.# Please use &lt;!-- more --&gt; in the post to control excerpt accurately.auto_excerpt: enable: false length: 150 把enable 的false改成true就行了，然后length是设定文章预览的文本长度。 修改后重启 hexo 就ok了。 2. 方法一：手动显示 在你写 md 文章的时候，可以在内容中加上 &lt;!--more--&gt;，这样首页和列表页展示的文章内容就是 &lt;!--more--&gt;之前的文字，而之后的就不会显示了。 文章更新时间 hexo添加文章更新时间 自动推送 将主题配置文件中的baidu_push设置为true 修改文章底部的那个带#号的标签 具体实现方法 修改模板/themes/next/layout/_macro/post.swig，搜索 rel=&quot;tag&quot;&gt;#，将 # 换成&lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt; 主页文章添加阴影效果 具体实现方法 * 打开\themes\next\source\css\_custom\custom.styl,向里面加入： 12345678// 主页文章添加阴影效果 .post &#123; margin-top: 60px; margin-bottom: 60px; padding: 25px; -webkit-box-shadow: 0 0 5px rgba(202, 203, 203, .5); -moz-box-shadow: 0 0 5px rgba(202, 203, 204, .5); &#125; 实现文章统计功能 在根目录下安装 hexo-wordcount运行 : 1$ cnpm install hexo-wordcount --save 然后在主题的配置文件中，配置如下： 123456# Post wordcount display settings# Dependencies: https://github.com/willin/hexo-wordcountpost_wordcount: item_text: true wordcount: true min2read: false 优化 找到Blog.swig 文件 搜到找到如下代码 * 注：(个人出现的markdown语法错误)这里会出现markdowm的语法错误，原因是wordcount()被当成特殊字符，详细请看 改为图片显示为 另一个错误是该插件版本与node.js版本不兼容，hexo g时会报如下错误 12345ERROR Plugin load failed: hexo-wordcountError: Cannot find module 'numeral' at Function.Module._resolveFilename (module.js:489:15) at Function.Module._load (module.js:439:25) at Module.require (module.js:517:17) 指定老的版本即可：cnpm install hexo-wordcount@2 --save 数学符号渲染 hexo有自带的数学符号渲染方法mathjax 在主题中只需要开启mathjax开关 eg：在next主题目录的_config.yml配置中,把mathjax默认的false修改为true 1234# MathJax Supportmathjax: enable: true per_page: true 在文章的Front-matter中打开mathjax开关(mathjax加载耗资源，只在需要的文章中加载) 123456---title: index.htmldate: tags:mathjax: true--- 评论系统 来必力(韩国人的，不怎么喜欢) gitment(推荐，Next主题下以集成gitment) 可能产生的错误 Hexo+gitment的Error：validation failed Hexo+gitment的Error：validation failed 其他错误 Gitment评论功能接入踩坑教程 一般都是路径的配置有问题，如搞不清楚各个配置的含义，这篇文章讲得蛮清楚了 错误集合 1Error: Cannot find module 'math-work' cmd中cnpm install math-work 参考的网站 主题 优化 next主题优化 Hexo博客优化，如何加快你的博客访问速度 其他 官方文档 在Hexo中渲染MathJax数学公式 hexo 摸爬滚打之进阶教程]]></content>
      <categories>
        <category>Hexo建站</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello Hexo(基本配置)]]></title>
    <url>%2F2017%2F12%2F04%2Fhexo%E7%9B%B8%E5%85%B3%2FHello-Hexo%2F</url>
    <content type="text"><![CDATA[想法 看到人家的博客界面很漂亮，最近学习压力又大，就想自己写一个啦 看到是用node.js的一个框架Hexo，简单又漂亮，还可以放在github上直接打开博客，那就直接上手吧。 hexo文档链接 安装 首先安装依赖环境 搭建个人博客-hexo+github详细完整步骤 hexo安装失败(地址被“墙”)请参考-零基础免费搭建个人博客-hexo+github 注：如用淘宝安装请改为cnpm install 主题(可选) 人靠衣装马靠鞍，来，加上这个好看的next主题来一波特技 在博客根目录下克隆next主题 1git clone https://github.com/iissnan/hexo-theme-next themes/next 然后在hexo根目录下的配置文件_config.yml中把 theme 的主题改为next（theme字段在配置的最后面） 1theme: next 然后就是我们的好习惯（其实只换了主题文章没换的话只要-s就好） 12hexo ghexo s 然后就登录http://localhost:4000/ 查看效果（然后你会发现博客打开速度变得超快，后面我们再继续优化） next提供了四个主题风格选择，个人喜好Gemini风格 步骤是：打开next下的配置文件_config.yml，找到这一段选择主题 123456789# ---------------------------------------------------------------# Scheme Settings# ---------------------------------------------------------------# Schemes#scheme: Muse#scheme: Mist#scheme: Piscesscheme: Gemini next主题的官方文档 作者是国人，官方文档写得都很详细，传送门 把博客静态文件上传到github或coding上 把源代码上传到coding.net上(可选) 这里主要做的是把网页与源码分开，原因如下 coding免费提供私人仓库，增加安全性(如私人未公开博客) 上传源代码，可以确保博客完整不丢失，使更新方便(在其他电脑上也可以随时更新博客) 添加密匙 找到之前github生成的密匙 1$cd ~/.ssh 运行如下命令，将公钥的内容复制到系统粘贴板(clipboard)中 1clip &lt; ~/.ssh/id_rsa.pub 然后放到coding个人配置的ssh中就ok啦 Hexo中上传到github 删除根目录和主题目录下的.git文件夹。 修改根目录下的.gitignore文件为：/.deploy_git/public其实第一行留不留都一样，它是hexo默认的git配置文件夹，里面也有一个.git，使/.deploy_git里的文件无法被提交。public是每次hexo g新生成的静态博客文件，不需要同步。每次写完博客，hexo d之后再push一次就行 其他 swig文件中使用注释 `` 相关资料 * hexo干货系列 * GoodHexo使用常见问题及解决办法 * 更新next到远程]]></content>
      <categories>
        <category>Hexo建站</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NumPy（索引、array合并与分割）]]></title>
    <url>%2F2017%2F12%2F04%2FPython%E5%9F%BA%E7%A1%80%2FNumPy%EF%BC%88%E7%B4%A2%E5%BC%95%E3%80%81array%E5%90%88%E5%B9%B6%E4%B8%8E%E5%88%86%E5%89%B2%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一、Numpy索引 一维索引 12345import numpy as npA = np.arange(3,15)# array([3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]) print(A[3]) # 6 二维索引 12345678A = np.arange(3,15).reshape((3,4))"""array([[ 3, 4, 5, 6] [ 7, 8, 9, 10] [11, 12, 13, 14]])""" print(A[2]) # [11 12 13 14] 二维索引取值 1print(A[1][1]) # 8 切片操作 注：1:3表示对第2到第4进行切片输出（不包含第4） 12print(A[1, 1]) # 8print(A[1, 1:3]) # [8 9] 二、array 合并 np.vstack() 上下合并 vertical stack本身属于一种上下合并，即对括号中的两个整体进行对应操作。 123456789import numpy as npA = np.array([1,1,1])B = np.array([2,2,2]) print(np.vstack((A,B))) # vertical stack"""[[1,1,1] [2,2,2]]""" 利用shape函数可以让我们很容易地知道A和C的属性，从打印出的结果来看，A仅仅是一个拥有3项元素的数组（数列），而合并后得到的C是一个2行3列的矩阵。 123C = np.vstack((A,B)) print(A.shape,C.shape)# (3,) (2,3) np.hstack() 左右合并 1234567D = np.hstack((A,B)) # horizontal stackprint(D)# [1,1,1,2,2,2]print(A.shape,D.shape)# (3,) (6,) 通过打印出的结果可以看出：D本身来源于A，B两个数列的左右合并，而且新生成的D本身也是一个含有6项元素的序列。]]></content>
      <categories>
        <category>Python基础</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NumPy（矩阵运算）]]></title>
    <url>%2F2017%2F12%2F02%2FPython%E5%9F%BA%E7%A1%80%2FNumPy%EF%BC%88%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一、基本运算 基础加法、减、乘 123456import numpy as npa=np.array([10,20,30,40]) # array([10, 20, 30, 40])b=np.arange(4) # array([0, 1, 2, 3])c=a+b # array([10, 21, 32, 43])c=a-b # array([10, 19, 28, 37])c=a*b # array([ 0, 20, 60, 120]) 矩阵的乘法，针对的是第一个矩阵的行与第二个矩阵的列 umpy中，想要求出矩阵中各个元素的乘方需要依赖双星符号 **，以二次方举例 1c=b**2 # array([0, 1, 4, 9]) 逻辑判断 在脚本中对print函数进行一些修改可以进行逻辑判断： 12print(b&lt;3) # array([ True, True, True, False], dtype=bool) 二、矩阵操作函数 矩阵乘积 NumPy 的 dot 函数和matmul函数 二维矩阵下dot 和 matmul 函数的结果是相同的 12c = np.dot (a, b)c = np.matmul(a, b) 展开矩阵 flatten 将多维的矩阵进行展开成1行的数列 lat是一个迭代器，本身是一个object属性。 12345678print(A.flatten())# array([3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])for item in A.flat: print(item)# 3# 4……# 14 对矩阵的值进行操作 最大最小，总值 123np.sum(A) # 4.4043622002745959np.min(A) # 0.23651223533671784np.max(A) # 0.90438450240606416 求均值的几种方法 12np.average(A) np.mean(A) 或 A.mean() 求解中位数 1A.median() Cumsum累加 在cumsum()函数中：生成的每一项矩阵元素均是从原矩阵首项累加到对应项的元素之和。比如元素9，在cumsum()生成的矩阵中序号为3，即原矩阵中2，3，4三个元素的和。 1np.cumsum(A) # 累加 diff累差 该函数计算的便是每一行中后一项与前一项之差。故一个3行4列矩阵通过函数计算得到的矩阵便是3行3列的矩阵。 1np.diff(A) # 累差 nonzero()函数 官方：numpy函数返回非零元素的目录。 返回值为元组， 两个值分别为两个维度， 包含了相应维度上非零元素的目录值。 可以通过a[nonzero(a)]来获得所有非零值。 个人：nonzero(a) 将对矩阵a的所有非零元素， 将其分为两个维度， 返回其在各维度上的目录值。 123456789如果 a=mat([ [1,0,0], [0,0,0], [0,0,0]]) 则 nonzero(a) 返回值为 (array([0]), array([0])) , 因为矩阵a只有一个非零值， 在第0行， 第0列。如果 a=mat([ [1,0,0], [1,0,0], [0,0,0]]) 则 nonzero(a) 返回值为 (array([0, 1]), array([0, 0])) , 因为矩阵a只有两个非零值， 在第0行、第0列，和第1行、第0列。所以结果元组中，第一个行维度数据为（0,1） 元组第二个列维度都为（0,0）。 sort排序函数 对所有元素进行仿照列表一样的排序操作，但仅针对每一行进行从小到大排序操作： 123456789101112import numpy as npA = np.arange(14,2, -1).reshape((3,4)) # array([[14, 13, 12, 11],# [10, 9, 8, 7],# [ 6, 5, 4, 3]])print(np.sort(A)) # array([[11,12,13,14]# [ 7, 8, 9,10]# [ 3, 4, 5, 6]]) 三、矩阵转置 对axis进行赋值 当axis的值为0的时候，将会以列作为查找单元， 当axis的值为1的时候，将会以行作为查找单元。 123print("sum =",np.sum(a,axis=1))print("min =",np.min(a,axis=0))print("max =",np.max(a,axis=1)) 矩阵的转置有两种表示方法： 12print(np.transpose(A)) print(A.T) 特殊转置clip()函数 这个函数的格式是clip(Array,Array_min,Array_max)，顾名思义，Array指的是将要被执行用的矩阵，而后面的最小值最大值则用于让函数判断矩阵中元素，将矩阵中比最小值小的或者比最大值大的元素转换为最小值或者最大值。 123456789print(A)# array([[14,13,12,11]# [10, 9, 8, 7]# [ 6, 5, 4, 3]])print(np.clip(A,5,9)) # array([[ 9, 9, 9, 9]# [ 9, 9, 8, 7]# [ 6, 5, 5, 5]])]]></content>
      <categories>
        <category>Python基础</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NumPy（基础）]]></title>
    <url>%2F2017%2F12%2F01%2FPython%E5%9F%BA%E7%A1%80%2FNumPy%EF%BC%88%E5%9F%BA%E7%A1%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[常用函数 mat( ): 将数组转化为矩阵 np.I 操作符: 实现了矩阵求逆的运算 np.log:是计算对数函数 np.abss:是计算数据的绝对值 np.max imum:计算元素 y 中的最大值，你也可以 np.max imum(v,0) 数据类型和形状 NumPy 中处理数字的最常见方式是通过ndarray对象。它们与 Python 列表相似，但是可以有任意数量的维度。而且，ndarray支持快速的数学运算，这正是我们想要的。 由于它可以存储任意数量的维度，你可以使用ndarray来表示我们之前提到的任意数据类型：标量、向量、矩阵或张量。 12#为了方便使用numpy 采用np简写import numpy as np numpy 的几种属性（维度、行数和列数、元素个数） 123456array = np.array([[1,2,3],[2,3,4]]) #列表转化为矩阵print(array)"""array([[1, 2, 3], [2, 3, 4]])""" 12345678print('number of dim:',array.ndim) # 维度# number of dim: 2print('shape :',array.shape) # 行数和列数# shape : (2, 3)print('size:',array.size) # 元素个数# size: 6 创建 array array：创建数组 123a = np.array([2,23,4]) # list 1dprint(a)# [2 23 4] 指定数据 dtype * 当你创建 NumPy 数组时，可以指定类型 - 但是数组中的每一项必须具有相同的类型。 123456789101112131415# py3.5后创建int默认类型为int32a = np.array([2,23,4],dtype=np.int)print(a.dtype)# int64a = np.array([2,23,4],dtype=np.int64)print(a.dtype)# 创建float默认类型为float64a = np.array([2,23,4],dtype=np.float)print(a.dtype)# float32a = np.array([2,23,4],dtype=np.float32)print(a.dtype) 创建特定数据 2行3列矩阵 123456a = np.array([[2,23,4],[2,32,4]]) # 2d 矩阵 2行3列print(a)"""[[ 2 23 4] [ 2 32 4]]""" 创建全零数组 123456a = np.zeros((3,4)) # 数据全为0，3行4列"""array([[ 0., 0., 0., 0.], [ 0., 0., 0., 0.], [ 0., 0., 0., 0.]])""" 创建全一数组, 同时也能指定这些特定数据的 dtype: 123456a = np.ones((3,4),dtype = np.int) # 数据为1，3行4列"""array([[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]])""" 创建全空数组, 其实每个值都是接近于零的数: 123456789a = np.empty((3,4)) # 数据为empty，3行4列"""array([[ 0.00000000e+000, 4.94065646e-324, 9.88131292e-324, 1.48219694e-323], [ 1.97626258e-323, 2.47032823e-323, 2.96439388e-323, 3.45845952e-323], [ 3.95252517e-323, 4.44659081e-323, 4.94065646e-323, 5.43472210e-323]])""" 用 arange 创建连续数组: 1234a = np.arange(10,20,2) # 10-19 的数据，2步长"""array([10, 12, 14, 16, 18])""" 使用 reshape 改变数据的形状 123456a = np.arange(12).reshape((3,4)) # 3行4列，0到11"""array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]])""" 用 linspace 创建线段型数据: 12345678a = np.linspace(1,10,20) # 开始端1，结束端10，且分割成20个数据，生成线段"""array([ 1. , 1.47368421, 1.94736842, 2.42105263, 2.89473684, 3.36842105, 3.84210526, 4.31578947, 4.78947368, 5.26315789, 5.73684211, 6.21052632, 6.68421053, 7.15789474, 7.63157895, 8.10526316, 8.57894737, 9.05263158, 9.52631579, 10. ])""" 同样也能进行 reshape 工作: 12345678a = np.linspace(1,10,20).reshape((5,4)) # 更改shape"""array([[ 1. , 1.47368421, 1.94736842, 2.42105263], [ 2.89473684, 3.36842105, 3.84210526, 4.31578947], [ 4.78947368, 5.26315789, 5.73684211, 6.21052632], [ 6.68421053, 7.15789474, 7.63157895, 8.10526316], [ 8.57894737, 9.05263158, 9.52631579, 10. ]])"""]]></content>
      <categories>
        <category>Python基础</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第三章-浅层神经网络]]></title>
    <url>%2F2017%2F11%2F21%2F%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2Fw%E7%AC%AC%E4%B8%89%E7%AB%A0-%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[概论 逻辑回归 首先你需要输入特征 x，参数W 和b ，通过\(z = w^T x + b\)你就可以计算 出 z ，接下来使用\(a=σ(z)\)就可以计算出a。 我们将a的符号换为表示输出的 \(\hat{y}\)，然后可以计算出损失函数\(ℒ(a,y)\)，进而不断采用梯度下降法找到参数w和b的最优解。 采用这种算法编写的猫识别器最终的准确率只有70%，想要进一步提高识别的精准度，就需要建立起一个多层的神经网络来训练样本。 ### 基本神经网络结构 首先\(X\)表示输入特征，\(W\) 和 \(b\) 是参数，\(z^{[1]} = w^{[1]}X + b^{[1]}\)计算出 \(z^{[1]}\)。 使用\(x^{(i)}\)表示第 i 个训练样本，上标方括号 1、2 表示不同的层，图中[1]表示神经网络的第一层， [2]表示神经网络的第二层 。 类似逻辑回归，在计算\(z^{[1]}\)后需要使用\(σ(z^{[1]})\)计算 \(a^{[1]}\) 接下来你需要使用另外一个线 \(z^{[2]} = W^{[2]} a^{[1]} + b^{[2]}\)计算 \(z^{[2]}\)，\(a^{[2]}= σ(z^{[2]})\)计算\(a^{[2]}\) ，此时\(a^{[2]}\) 就是整个神经网络最终的输出，用 𝑦表示网络的输出。 反向计算 神经网络表示 神经网络的符号惯例： \(x\) 表示输入特征 \(a\) 表示每个神经元的输出， \(w\) 表示特征的权重， 上标表示神经网络的层数（隐藏层为 1），下标表示该层的第几个神经元。 结构 输入层 隐藏层（很好解释了之前在第一章的疑惑）：在一个神经网络中，当你使用监督学习训练它的时候，训练集包含了输入 也包含了目标输出 ，所以术语隐藏层的含义是在训练集中，这些中间结点的准确值我们是不知道到的，也就是说你看不见它们在训练集中应具有的值。你能看见输入的值，你也能看见输出的值，但是隐藏层中的东西，在训练集中你是无法看到的。所以这也解释了词语隐藏层，只是表示你无法在训练集中看到他们。 简单点说，隐藏层就是把中间处理的过程隐藏起来而没有显性显示出来。 输出层 数据表示 隐藏层以及最后的输出层是带有参数的，参数个数与隐藏层单元/节点有关 传统上（两层的神经网络）：输入层是不算入总层数内，所以隐藏层是第一层，输出层是第二层。 技术上（三层的神经网络）：因为这里有输入层、隐藏层，还有输出层。 神经网络的输出 * 注：\(z\)代表隐藏层，这里有两个隐藏层 多样本向量化 多个训练样本，则重复计算每个样本的4个方程（for循环） 利用向量化简化该过程 水平方向，这个垂直索引对应于神经网络中的不同的训练样本。 垂直方向，这个垂直索引对应于神经网络中的不同节点。 向 量 化 实 现 的 解 释 激活函数（ Activation functions） 建立一个神经网络时，需要关心的一个问题是，在每个不同的独立层中应当采用哪种激活函数。 sigmoid函数 逻辑回归中，一直采用sigmoid函数作为激活函数，此外还有一些更好的选择。 tanh函数 表达式 \[tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}\] 函数图像 tanh（Hyperbolic Tangent Function，双曲正切函数）其实是sigmoid函数的移位版本。对于隐藏单元，选用tanh函数作为激活函数的话，效果总比sigmoid函数好，因为tanh函数的值在-1到1之间，最后输出的结果的平均值更趋近于0，而不是采用sigmoid函数时的0.5，这实际上可以使得下一层的学习变得更加轻松。 对于二分类问题，为确保输出在0到1之间，将仍然采用sigmiod函数作为输出的激活函数。 缺点 然而sigmoid函数和tanh函数都具有的缺点之一是，在z接近无穷大或无穷小时，这两个函数的导数也就是梯度变得非常小，此时梯度下降的速度也会变得非常慢。 线性修正单元（ReLU函数） 表达式 \[g(z) = max(0,z) =\begin{cases} 0, &amp; \text{($z$ $\le$ 0)} \\ z, &amp; \text{($z$ $\gt$ 0)} \end{cases}\] 函数图像 mark 特点 当z大于0时是，ReLU函数的导数一直为1，所以采用ReLU函数作为激活函数时，随机梯度下降的收敛速度会比sigmoid及tanh快得多，但负数轴的数据都丢失了。 修正版本（Leaky-ReLU） 表达式 \[g(z) = max(0,z) =\begin{cases} \alpha z, &amp; \text{($z$ $\le$ 0)} \\ z, &amp; \text{($z$ $\gt$ 0)} \end{cases}\] 函数图像 其中\(\alpha\)是一个很小的常数，用来保留一部非负数轴的值。 注意 可以发现，以上所述的几种激活函数都是非线性的，原因在于使用线性的激活函数时，输出结果将是输入的线性组合，这样的话使用神经网络与直接使用线性模型的效果相当，此时神经网络就类似于一个简单的逻辑回归模型，失去了其本身的优势和价值。 代码 课程代码-GitHub 参考资料 吴恩达-神经网络与深度学习-网易云课堂 Deep Learning系列课程资料笔记]]></content>
      <categories>
        <category>深度学习与神经网络(吴恩达)</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[章二习题（NumPy基础、广播和向量化）]]></title>
    <url>%2F2017%2F11%2F14%2F%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2Fw%E7%AB%A0%E4%BA%8C%E4%B9%A0%E9%A2%98%EF%BC%88NumPy%E5%9F%BA%E7%A1%80%E3%80%81%E5%B9%BF%E6%92%AD%E5%92%8C%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%89%2F</url>
    <content type="text"><![CDATA[numpy常用函数 mat( ): 将数组转化为矩阵 np.I 操作符: 实现了矩阵求逆的运算 np.log:是计算对数函数 np.abss:是计算数据的绝对值 np.max imum:计算元素 y 中的最大值，你也可以 np.max imum(v,0) 1. np.exp() 使用np.exp（） 12345import numpy as np# example of np.expx = np.array([1, 2, 3])print(np.exp(x)) # result is (exp(1), exp(2), exp(3)) 1[2.71828183 7.3890561 20.08553692] 实现sigmoid函数 1234567891011121314import numpy as np def sigmoid(x): """ Arguments: x -- A scalar or numpy array of any size # x-- 任何大小的标量或numpy数组 Return: s -- sigmoid(x) """ s = 1 / (1 + np.exp(-x)) return s 12x = np.array([1, 2, 3])sigmoid(x) Sigmoid梯度 计算梯度已使用反向传播优化损失函数 12345678910111213141516171819mport numpy as np def sigmoid(x): """ 计算sigmoid函数相对于其输入x的梯度(斜率或导数) 可以将sigmoid函数的输出存储到变量中，然后使用它来计算渐变 Arguments: x -- A scalar or numpy array of any size # x-- 任何大小的标量或numpy数组 Return: ds -- 你的计算渐变 """ s = 1 / (1 + np.exp(-x)) ds = s * (1 - s) return ds 2.X.shape和X.reshape X.shape：用于获取矩阵/向量X的形状（维度） X.reshape：用于将X重塑为其他维度 3.标准化处理 实现normalizeRows（）来规范矩阵的行。将这个函数应用到输入矩阵x之后，x的每一行应该是单位长度的向量（意思是长度1） 12345678910111213141516171819202122# 分级功能: normalizeRowsdef normalizeRows(x): """ 实现对矩阵x的每一行进行归一化的函数（具有单位长度） Argument: x -- A numpy matrix of shape (n, m) Returns: x -- The normalized (by row) numpy matrix. You are allowed to modify x. x -- 标准化（按行）numpy矩阵，可以修改x """ # 计算 x_norm 作为 x的规范2. 使用 np.linalg.norm(..., ord = 2, axis = ..., keepdims = True) x_norm = np.linalg.norm(x, axis = 1, keepdims = True) # Divide x by its norm. # 将x除以其规范 x = x / x_norm return x 1234x = np.array([ [0, 3, 4], [1, 6, 4]])print("normalizeRows(x) = " + str(normalizeRows(x))) 12normalizeRows(x) = [[ 0. 0.6 0.8 ] [ 0.13736056 0.82416338 0.54944226]] 4.广播和softmax函数 使用numpy实现softmax函数：归一类函数，对两个或多个类进行分类时使用 123456789101112131415161718192021222324252627def softmax(x): """计算输入x的每一行softmax row vector 行矢量 matrices 形状 Your code should work for a row vector and also for matrices of shape (n, m). Argument: x -- A numpy matrix of shape (n,m) Returns: s -- A numpy matrix equal to the softmax of x, of shape (n,m) 返回等于x的softmax的numpy矩阵，形状（n,m） """ # element-wise 元素 # Apply exp() element-wise to x. Use np.exp(...). x_exp = np.exp(x) # Create a vector x_sum that sums each row of x_exp. 对x_exp的每一行求和 # Use np.sum(..., axis = 1, keepdims = True). x_sum = np.sum(x_exp, axis = 1, keepdims = True) # dividing 除以 # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting. 自动使用numpy广播 s = x_exp / x_sum return s 1234x = np.array([ [9, 2, 5, 0, 0], [7, 5, 0, 0 ,0]])print("softmax(x) = " + str(softmax(x))) np.exp（x）适用于任何np.array x，并将指数函数应用于每个坐标 sigmoid函数及其渐变 image2vector常用于深度学习 np.reshape被广泛使用。在将来，你会发现保持矩阵/向量的维度直接将消除大量的错误。 numpy具有高效的内置功能 广播是非常有用的 5.向量化(Vectorization) 在深度学习中，您处理非常大的数据集。因此，非计算最优函数可能会成为算法中的一个巨大瓶颈，并可能导致需要长时间运行的模型。为了确保你的代码在计算上是高效的，你将使用向量化。 123456789101112131415161718192021222324252627282930313233343536373839import timex1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]# DOT PRODUCT 矢量积### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###tic = time.process_time()dot = 0for i in range(len(x1)): dot+= x1[i]*x2[i]toc = time.process_time()print ("dot = " + str(dot) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")### CLASSIC OUTER PRODUCT IMPLEMENTATION ###tic = time.process_time()outer = np.zeros((len(x1),len(x2))) # we create a len(x1)*len(x2) matrix with only zerosfor i in range(len(x1)): for j in range(len(x2)): outer[i,j] = x1[i]*x2[j]toc = time.process_time()print ("outer = " + str(outer) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")### CLASSIC ELEMENTWISE IMPLEMENTATION ###tic = time.process_time()mul = np.zeros(len(x1))for i in range(len(x1)): mul[i] = x1[i]*x2[i]toc = time.process_time()print ("elementwise multiplication = " + str(mul) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###W = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy arraytic = time.process_time()gdot = np.zeros(W.shape[0])for i in range(W.shape[0]): for j in range(len(x1)): gdot[i] += W[i,j]*x1[j]toc = time.process_time()print ("gdot = " + str(gdot) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms") 1234567891011121314151617181920212223242526x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]### VECTORIZED DOT PRODUCT OF VECTORS ###tic = time.process_time()dot = np.dot(x1,x2)toc = time.process_time()print ("dot = " + str(dot) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")### VECTORIZED OUTER PRODUCT ###tic = time.process_time()outer = np.outer(x1,x2)toc = time.process_time()print ("outer = " + str(outer) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")### VECTORIZED ELEMENTWISE MULTIPLICATION ###tic = time.process_time()mul = np.multiply(x1,x2)toc = time.process_time()print ("elementwise multiplication = " + str(mul) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")### VECTORIZED GENERAL DOT PRODUCT ###tic = time.process_time()dot = np.dot(W,x1)toc = time.process_time()print ("gdot = " + str(dot) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms") 实现L1和L2的损失函数 实现L1损失函数的numpy向量化版本 123456789101112131415def L1(yhat, y): """ Arguments: predicted labels 预测标签 yhat -- vector of size m (predicted labels) y -- vector of size m (true labels) Returns: defined above 上面定义 loss -- the value of the L1 loss function defined above """ loss = np.sum(np.abs(y - yhat)) return loss 123yhat = np.array([.9, 0.2, 0.1, .4, .9])y = np.array([1, 0, 0, 1, 1])print("L1 = " + str(L1(yhat,y))) 1L1 = 1.1 实现L2损失函数的numpy向量化版本 12345678910111213def L2(yhat, y): """ Arguments: yhat -- vector of size m (predicted labels) y -- vector of size m (true labels) Returns: loss -- the value of the L2 loss function defined above """ loss = np.dot((y - yhat),(y - yhat).T) return loss 123yhat = np.array([.9, 0.2, 0.1, .4, .9])y = np.array([1, 0, 0, 1, 1])print("L2 = " + str(L2(yhat,y))) 1L2 = 0.43 矢量化在深度学习中非常重要。它提供了计算效率和清晰度。 你已经检查了L1和L2的损失。 您熟悉np.sum，np.dot，np.multiply，np.maximum等许多numpy函数。]]></content>
      <categories>
        <category>深度学习与神经网络(吴恩达)</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第二章-神经网络基础（logistic 回归函数、向量化）]]></title>
    <url>%2F2017%2F11%2F09%2F%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2Fw%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%EF%BC%88logistic%20%E5%9B%9E%E5%BD%92%E5%87%BD%E6%95%B0%E3%80%81%E5%90%91%E9%87%8F%E5%8C%96%EF%BC%89%2F</url>
    <content type="text"><![CDATA[2.1 二分分类 计算机保存一张图片，要保存三个独立矩阵，分别对应红、绿、蓝三个颜色通道 举例：图为一张猫图，像素为 64x64， 就有三个 64x64 的矩阵 用特征向量 x 来表示图片 则图片的维度n = nx = 64 * 64 * 3 = 12 288 在二分类问题中，目标是训练出一个分类器（x 作为输入，预测输出的结果标签为 y） 上图表现的是x是nx维的特征向量 训练集由m个训练样本构成 X矩阵式m列 高为nx （训练样本作为列向量堆叠）（约定格式） 注： 也有(训练样本作为行向量堆叠）的矩阵形式，但在神经网络中一般用约定格式 2.2 logistic 回归模型 course2-3.jpg 线性回归中可用 y = w^t * x + b 缺点： y 应该介于0和1之间 可用sigmoid(z)函数，也就是上图中的G(z) 对于参数 w 和 b ： b 对应一个拦截器 2.3 logistic 回归损失函数（成本函数） 损失函数L(^y,y)： 在单个训练样本中定义的，衡量了在单个训练样本上的表现 成本函数(代价函数) J（w，b）：衡量了在==全部训练样本==上的表现（在logistic函数中寻找合适的 w 和 b 让 y-hat 尽可能的小） 2.4 阶梯下降法 course2-5.jpg 成本函数 J （w，b）是一个凸函数（选择该函数的原因） 阶梯下降法的做法是： 从初始点开始，朝最陡的下坡方向走一步 2.5 向量化 注：当你想写循环时，可以先检查numpy中是否有类似的内置函数 ==向量化速度比循环快300倍==（真是个神奇的东西） 1234567891011121314151617181920import timea = np.random.rand(1000000)b = np.random.rand(1000000)tic = time.time()c = np.dot(a,b)toc = time.time()print(c)print("Vectorized version(向量化):" + str(1000*(toc-tic))+"ms")c = 0tic = time.time()for i in range(1000000): c += a[i]*b[i]toc = time.time()print(c)print("For loop(循环):" + str(1000*(toc-tic))+ "ms") 12345# 输出249961.060873Vectorized version(向量化):2.0003318786621094ms249961.060873For loop(循环):779.0098190307617ms np.dot(W,X)： eg: 计算Z = W^T+ b,向量化可以直接计算w^T(表示W的转置) 1Z = np.dot(W.T,X)+b 2.6 高度向量化的、非常高效的逻辑回归的梯度下降算法 1234567Z = wTX + b = np. dot(w. T, X)A = σ(Z)dZ = A -Ydw = 1/m ∗ X ∗ dz^Tdb = 1/m * np. sum(dZ)w = w - α * dwb = b - α * db 我们利用前五个公式完成了前向和后向传播，也实现了对所有训练样本进行预测和求 导 利用后两个公式，梯度下降更新参数。 我们的目的是不使用for循环，所以我们就通过一次迭代实现一次梯度下降，但如果你希望多次迭代进行梯度下降，那么仍然需要for循环，放在最外层。不过我们还是觉得一次迭代就进行一次梯度下降，避免使用任何循环比较 舒服一些。 2.7 python中的广播机制( Broadcasting in Python) 12345import numpy as npA = np.array([[56.0, 0.0, 4.4, 68.0], [1.2, 104.0, 52.0, 8.0], [1.8, 135.0, 99.0, 0.9]])print(A) 123[[ 56. 0. 4.4 68. ] [ 1.2 104. 52. 8. ] [ 1.8 135. 99. 0.9]] axis=0 意思是、在竖直方向上求和 axis=1 是在水平方向上求和 12cal = A.sum(axis=0)print(cal) 1[ 59. 239. 155.4 76.9] 12percentage = 100*A/cal.reshape(1,4)print(percentage) 123[[ 94.91525424 0. 2.83140283 88.42652796] [ 2.03389831 43.51464435 33.46203346 10.40312094] [ 3.05084746 56.48535565 63.70656371 1.17035111]] axis 用来指明将要进行的运算是沿着 哪个轴执行，在 numpy 中， 0 轴是垂直的，也就是列，而 1 轴是水平的，也就是行 A/cal.reshape(1,4)指令则调用了 numpy 中的广播机制。 这里使用3X4的矩阵 A 除以1X4 的矩阵 cal。技术上来讲，其实并不需要再将矩阵 cal reshape(重塑)成1X4 ，因 为矩阵 cal 本身已经是1X4 了。但是当我们写代码时不确定矩阵维度的时候，通常会对矩阵 进行重塑来确保得到我们想要的列向量或行向量。 重塑操作 reshape是一个常量时间的操作， 时间复杂度是 O(1)，它的调用代价极低。 numpy 广播机制 如果两个数组的后缘维度的轴长度相符或其中一方的轴长度为 1，则认为它们是广播兼容的。 广播机制与执行的运算种类无关 广播会在缺失维度和轴长度为 1 的维度上进行。 后缘维度的轴长度： A.shape[-1] 即矩阵维度元组中的最后一个位置的值]]></content>
      <categories>
        <category>深度学习与神经网络(吴恩达)</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实战四（神经网络）]]></title>
    <url>%2F2017%2F11%2F08%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E5%9B%9B%EF%BC%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[* NumPy 是Python语言的一个扩充程序库。支持高级大量的维度数组与矩阵运算，此外也针对数组运算提供大量的数学函数库。 二、单层神经元 1. 问题描述 问题 假设在一群人中，我们只能获得每个人的三个特征： 特征1：长发（1）还是短发（0） 特征2：衣服颜色是红色（1）还是不是红色（0） 特征3：身高大于178cm（1）还是不超过178（0） 假设我们只知道其中四个人的性别（男：0，女：1），我们需要依据这四个人的三个特征以及性别训练一个神经网络，用于预测一个人的性别。 因此该问题有三个输入，一个输出。 样本信息如下： 头发 衣服 身高 性别 0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 2. 简单的单层神经网络 用X表示输入的特征向量，由于每个样本有三个特征，一共有四个样本，所以我们定义一个4X3的矩阵，每一行代表一个样本 1234567#import numpyimport numpy as np# input datasetX = np.array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]]) 而四个样本对应输出（分类结果）我们用一个1X4的矩阵表示。.T为转置函数，转置后变成了4X1的矩阵。 同我们的输入一致，每一行是一个训练实例，而每一列（仅有一列）对应一个输出节点。 1y = np.array([[0, 0, 1, 1]]).T 初始化神经网络的权重 由于输入层有三个神经元，而输出结果只有一个神经元，所以权重矩阵为3x1。 由于一般初始化权重是随机选择的，因此要为随机数设定产生的种子（如下代码所示） 这样可以使每次训练开始时，得到的训练随机数都是一致的。这样便于观察策略变动是如何影响网络训练的，消除初始权重的影响。 1np.random.seed(1) 由于我们要将随机初始化的权重矩阵均值设定为0，因此要计算syn0(第一层网络间的权重矩阵) 1syn0 = 2*np.random.random((3,1)) - 1 sigmoid函数 定义为： 特点：其导数可以用其自身表示出来，在计算的时候，我们只需要计算出其函数值，就可以计算出其导数值，从而可以减少浮点运算次数，提高效率 导数为： python中代码实现 12345# sigmoid functiondef nonlin(x,deriv=False): if(deriv==True): return x*(1-x) return 1/(1+np.exp(-x)) 其中，deriv参数表示是否计算的是其导数值： 3. 开始训练神经网络 迭代10000次，每一次迭代可描述如下 计算输入层的加权和，即用输入矩阵L0乘以权重矩阵syn0，并通过sigmid函数进行归一化。得到输出结果l1； 计算输出结果L1与真实结果y之间的误差L1_error； 计算权重矩阵的修正L1_delta，即用误差乘以sigmoid在L处的导数；（阶梯下降法） 用L1_delta更新权重矩阵syn0 1234567891011121314for iter in range(10000): # forward propagation l0 = X l1 = nonlin(np.dot(l0,syn0)) # how much did we miss? l1_error = y - l1 # multiply how much we missed by the # slope of the sigmoid at the values in l1 l1_delta = l1_error * nonlin(l1,True) # update weights syn0 += np.dot(l0.T,l1_delta) 4. 一次训练过程的参数更新如下图所示： image 批量循环训练 5.结果 1234Output syn0 After Training:[[ 9.67299303] [-0.2078435 ] [-4.62963669]] 从syn0迭代后的输出可看出 yn0的第一个元素，也就是第一个输入特征（长发）的权重最大，而第二个和第三个特征都很小 所以神经网络学习的结果是加重第一个特征的权重，而其他两个特征对于是女性这个推测的贡献较小，所以减小其权重。 为了验证训练结果，我们加入两组新数据，(短头发，红衣服，矮个子)，(长头发，不是红衣服，矮个子)，并用神经网络来进行分类： 123X_new = np.array([[0,1,0], [1,0,0]])y_new = np.dot(X_new,syn0) 计算结果如下： 123Predicte With syn0:[[-0.2078435 ] [ 9.67299303]] 完整代码 三、二层神经网络]]></content>
      <categories>
        <category>机器学习实战</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实战总结]]></title>
    <url>%2F2017%2F11%2F06%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[参考文章 学习路线 机器学习系列_机器学习路线图(附资料) 【重磅干货整理】机器学习(Machine Learning)与深度学习(Deep Learning)资料汇总：真的是超多干货 零.算法概念 监督学习：需要用已知结果的数据做训练 无监督学习：不需要已知标签 连续型数据和离散型数据 1.算法分类 1. 监督学习的用途 分类 k-邻近算法 朴素贝叶斯算法 支持向量机 决策树 回归 线性回归 逻辑回归 局部加权线性回归 Ridge回归 Lasso 最小回归系数估计 2. 无监督学习的用途 聚类和降维 K-均值 DBSCAN 最大期望算法 Parzcn窗设计 3. 特殊算法 推荐算法 4. 一些小方法（子算法） 梯度下降法：主要运用在线性回归，逻辑回归，神经网络，推荐算法中 牛顿法：主要运用在线性回归 BP算法：主要运用在神经网络 SMO算法：主要运用在SVM中 2.如何选择合适的算法 必须要考虑下面两个问题 使用算法的目的，想要算法完成何种任务 需要分析或收集的数据是什么 #### 基于目的考虑 想要预测目标的值，则选择监督学习算法，然后进一步确认目标变量的类型 离散型变量: 选择分类算法 连续型变量: 选择回归算法 否则选择无监督学习算法，随后进一步分析是否需要将数据分离为离散的组 不需要: 聚类算法 需要: 密度估计算法 基于数据考虑 特征值的类型 特征值是否缺失 数据是否存在异常值 特征发生的频率是否罕见 天下没有免费的午餐 没有哪个算法能在所有问题中都表现得最优秀，因此我们只能在一定程度上缩小算法的选择范围，尝试不同算法的执行效率，不断试错，优化算法。 3.基本算法优缺点 算法 优点 缺点 数据类型 优化方法 应用领域 K-邻近算法 精度高、对异常值不敏感、无数据输入假定 计算复杂度高、空间复杂度高，占用存储空间 数值型和标称型 文本分类、模式识别、聚类分析，多分类领域 决策树算法 1.能实现对未知数据进行高效分类 2.有较好的可读性和描述性，利于辅助人工分析 3.分类效率高，一次构建后可反复使用 1.难以处理连续的特征 2. 容易发生过拟合（随机森林可以很大程度上减少过拟合） 3.对于多分类问题，计算量和准确率都不理想 数值型和标称型 1、对决策树进行剪枝 2、使用基于决策树的combination算法来解决过拟合的问题 企业管理实践，企业投资决策，由于决策树很好的分析能力，在决策过程应用较多。 朴素贝叶斯 1.朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率。 2.对小规模的数据表现很好，能个处理多分类任务，适合增量式训练； 3.对缺失数据不太敏感，算法也比较简单，常用于文本分类。 1.需要计算先验概率 2.分类决策存在错误率 3.对输入数据的表达形式很敏感 标称型数据 文本分类、欺诈检测中使用较多 人工神经网络 1、分类准确度高，学习能力极强。 2、对噪声数据鲁棒性和容错性较强。 3、有联想能力，能逼近任意非线性关系。 1、神经网络参数较多，权值和阈值 2、黑盒过程，不能观察中间结果 3、学习过程比较长，有可能陷入局部极小值。 应用与计算机视觉，自然语言处理，语音识别等领域并取得很好的效果 3.适用框架 Input(x) Output (y) Application 框架 Home features Price Real Estate standard NN Ad，usr info click on ad?(0/1) Online Advertising standard NN Image Object(1,...,1000)（给照片打标签 Photo tagging CNN Audio Text transcript（输出文本） Speech recognition RNN English Chinese Machine translation RNNs Image,Radar info Position of other cars Autonomous driving custom Hybrid]]></content>
      <categories>
        <category>机器学习实战</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实战-Py3.X错误合集]]></title>
    <url>%2F2017%2F11%2F05%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-Py3.X%E9%94%99%E8%AF%AF%E5%90%88%E9%9B%86%2F</url>
    <content type="text"><![CDATA[零. 常见 1TypeError: 'range' object doesn't support item deletion 注：3.x中range()要改为list(rang())，因为python3中range不返回数组对象，而是返回range对象 1AttributeError: 'dict' object has no attribute 'iteritems' iteritems()要改为items() 二. kNN 报错： 1NameError: name 'reload' is not defined 在前面加入命令(个人推荐直接写在mian函数里面简单快捷) 1from imp import reload 四. 朴素贝叶斯 报错： 1UnicodeDecodeError: 'gbk' codec can't decode byte 0xae in position 199: illegal multibyte sequence 那是因为书上的下面这两行代码有点问题： 1234567wordList = textParse(open('email/spam/%d.txt' % i).read()wordList = textParse(open('email/ham/%d.txt' % i).read()需要将上面的代码更为下面这两行：wordList = textParse(open('email/spam/%d.txt' % i, "rb").read().decode('GBK','ignore') )wordList = textParse(open('email/ham/%d.txt' % i, "rb").read().decode('GBK','ignore') )因为有可能文件中存在类似“�”非法字符。 报错： 1TypeError: 'range' object doesn't support item deletion 1AttributeError: 'dict' object has no attribute 'iteritems' 参考常见错误 五. Logistic回归 报错： 1TypeError: 'numpy.float64' object cannot be interpreted as an integer 这里是因为numpy版本问题，更改版本解决 1pip install -U numpy==1.11.0 报错： 1TypeError: 'range' object doesn't support item deletion 参考常见错误 报错： 1AttributeError: 'numpy.ndarray' object has no attribute 'getA' 注释掉plotBestFit()的矩阵转为数组，因为在输入时已经转换为数组了 12345plotBestFit(weights)''' # 矩阵变为数组,使用gradAscent时加入 weights = wei.getA()''' 参考来自 机器学习实战Py3.x填坑记]]></content>
      <categories>
        <category>机器学习实战</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实战三（朴素贝叶斯）]]></title>
    <url>%2F2017%2F11%2F04%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E4%B8%89%EF%BC%88%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一、概述 1. 原理： 工作机制： 2. 优缺点 优点：在数据少的情况下有效，可以处理多类别问题 缺点：对于输入数据的准备方式较为敏感 适用数据范围：标称型数据 3.条件概论： 在B的条件下A出现的概率。 p(A|B)=p(AB)/p(B) 交换条件中的条件与结果： p(B|A)=p(A|B)*p(B)/p(A) 4.贝叶斯决策理论的核心思想： 选择具有最高概论的决策 5.朴素贝叶斯算法的两个假设： (1)每个特征之间都是独立的，这就使得公式： p((f1,f2,...fn)|c)=p(f1|c)p(f2|c)...p(fn|c) (2)每个特征同等重要，我们拿文本分类做例子，把文档中的单词作为特征。这种假设使得我们在进行分类的过程中无需考虑单词出现的次数，只考虑单词出现与否。这也就贝叶斯算法的贝努利模型实现方式。 注：贝叶斯的另一种实现方式为多项式模型，在这种模型中则需要考虑单词的出现次数。 二、算法流程 收集数据：可用任何方法 准备数据：需要数值型或者布尔型数据 分析数据：有大量特征时，绘制特征作用不大，此时使用直方图效果更好 训练算法：计算不同的独立特征的条件概率 测试算法：计算错误率 使用算法：一个常见的朴素贝叶斯应用是文档分类。可以在任意的分类场景中使用朴素贝叶斯分类器，不一定非要是文本。 三、算法实践 1.问题 对是否属于侮辱性文章进行分类 2.准备数据：从文本中构建词向量 准备数据 12345678910111213141516171819202122232425262728293031323334# 产生训练数据def loadDataSet(): # 该数据取自某狗狗论坛的留言版 postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']] # 标注每条数据的分类，这里0表示正常言论，1表示侮辱性留言 classVec = [0, 1, 0, 1, 0, 1] return postingList, classVec# 建立词汇表def createVocabList(dataSet): # 首先建立一个空集 vocabSet = set([]) # 遍历数据集中的每条数据 for document in dataSet: # 这条语句中首先统计了每条数据的词汇集，然后与总的词汇表求并集 vocabSet = vocabSet | set(document) return list(vocabSet)# 按照词汇表解析输入def setOfWords2Vec(vocabList, inputSet): # 创建一个跟词汇表（vocabList）等长的向量，并将其元素都设为0 returnVec = [0]*len(vocabList) # 遍历输入，将含有词汇表单词的文档向量设为1 for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)] = 1 else: print("the word:%s is not in my vocabulary!" % word) return returnVec 1234567命令行&gt;&gt;&gt; import bayes&gt;&gt;&gt; listOPosts,listClasses = bayes.loadDataSet()&gt;&gt;&gt; myVocabList = bayes.createVocabList(listOPosts)&gt;&gt;&gt; myVocabList&gt;&gt;&gt; bayes.setOfWords2Vec(myVocabList,listOPosts[0])&gt;&gt;&gt; bayes.setOfWords2Vec(myVocabList,listOPosts[3]) 3.训练算法：从词向量计算概率 1234567891011121314151617181920212223242526272829# 朴素贝叶斯分类器训练函数# 输入参数trainMatrix表示输入的文档矩阵，trainCategory表示每篇文档类别标签所构成的向量def trainNB0(trainMatrix,trainCategory): # 留言数目 numTrainDocs=len(trainMatrix) # 变换矩阵的列数目，即词汇表数目 numWords=len(trainMatrix[0]) # 侮辱性留言的概率 pAbusive=sum(trainCategory)/float(numTrainDocs) # 将所有词的出现数初始化为1，将分母初始化为2，从而降低计算多个概率的乘积结果为零的影响 p0Num=ones(numWords) p1Num=ones(numWords) p0Denom=2.0 p1Denom=2.0 for i in range(numTrainDocs): # 统计每类单词的数目，注意我们这里讨论的是一个二分问题 # 所以可以直接用一个if...else...即可，如果分类较多，则需要更改代码 if trainCategory[i] == 1: p1Num += trainMatrix[i] p1Denom += sum(trainMatrix[i]) else: p0Num += trainMatrix[i] p0Denom += sum(trainMatrix[i]) # 对每个类别除以该类中的总词数 # 防止下溢出 p1Vec = log(p1Num/p1Denom) p0Vec = log(p0Num/p0Denom) # 函数返回两个概率向量，及一个概率 return p0Vec, p1Vec, pAbusive 12345678910111213&gt;&gt;&gt; import bayes&gt;&gt;&gt; listOPosts,listClasses = bayes.loadDataSet()# 该语句从预先加载值中调入数据&gt;&gt;&gt; myVocabList = bayes.creatVocabList(listOPosts)&gt;&gt;&gt; trainMat=[]&gt;&gt;&gt; for postinDoc in listOPosts:... trainMat.append(bayes.setOfWords2Vec(myVocabList,postinDoc))...# 下面给出属于侮辱性文章的概论以及两个类别的概论向量&gt;&gt;&gt; p0V,p1V,pAb=bayes.trainNB0(trainMat,listClasses)&gt;&gt;&gt; pAb&gt;&gt;&gt; p0V&gt;&gt;&gt; p1V 4.测试算法: 朴素贝叶斯分类函数 12345678# 朴素贝叶斯分类函数def classifyNB(vec2Classify, p0Vec, p1Vec, pClass): p1 = sum(vec2Classify*p1Vec)+log(pClass) p0 = sum(vec2Classify*p0Vec)+log(1-pClass) if p1 &gt; p0: return 1 else: return 0 该函数是用来测试（封装了一些操作） 1234567891011121314#内嵌测试函数def testingNB(): listOPosts, listClasses=loadDataSet() myVocabList = createVocabList(listOPosts) trainMat = [] for postinDoc in listOPosts: trainMat.append(setOfWords2Vec(myVocabList, postinDoc)) p0V, p1V, p1 = trainNB0(trainMat, listClasses) testEntry = ['love', 'my', 'dalmation'] thisDoc = setOfWords2Vec(myVocabList, testEntry) print(testEntry, "classified as:", classifyNB(thisDoc, p0V, p1V, p1)) testEntry = ['garbage', 'stupid'] thisDoc = setOfWords2Vec(myVocabList, testEntry) print(testEntry, "classified as:", classifyNB(thisDoc, p0V, p1V, p1)) 12345命令行import bayes&gt;&gt;&gt; bayes.testingNB()['love', 'my', 'dalmation'] classified as: 0['garbage', 'stupid'] classified as: 1 5.词袋模型的转换函数(准备数据中优化) 之前的算法我们只考虑了单词出现与否，使用的是一种词集模型。 贝叶斯有两种实现方式，另一种多项式模型，需要考虑每个单词出现的次数，就是所谓的词袋模型。 为了适应这种词袋模型，我们需要对函数setOfWords2Vec作一下修改 1234567891011#词袋模型的转换函数 def bagOfWords2VecMN(vocabList,inputSet): returnVec=[0]*len(vocabList) #遍历输入 for word in inputSet: if word in vocabList: #现在每遇到一个单词会增加词向量中的对应量 returnVec[vocabList.index(word)]+=1 else: print "the word:%s is not in my vocabulary!" %word return returnVec 四、示例：使用朴素贝叶斯进行垃圾邮件过滤 1.准备数据，切分文本 12345# 该函数将每个句子都解析成单词，并忽略空格，标点符号以及长度小于3的单词def textParse(bigString): import re listOfTokens = re.split(r'\W*', bigString) return [tok.lower() for tok in listOfTokens if len(tok) &gt; 2] 2.分类器 错误信息合集（参考） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# 检测垃圾邮件def spamTest(): # 存放输入数据 docList = [] #存放类别标签 classList = [] # 所有的文本 fullText = [] # 分别读取邮件内容 for i in range(1, 26): wordList = textParse(open('email/spam/%d.txt' % i, "rb").read().decode('GBK','ignore') ) docList.append(wordList) fullText.extend(wordList) classList.append(1) wordList = textParse(open('email/ham/%d.txt' % i, "rb").read().decode('GBK','ignore') ) docList.append(wordList) fullText.extend(wordList) classList.append(0) vocabList = createVocabList(docList) # range(50)表示从0到50，不包括50 trainingSet = list(range(50)) # 测试集 testSet = [] # 随机抽取是个作为测试集 for i in range(10): # 从50个数据集中随机选取十个作为测试集，并把其从训练集中删除 randIndex = int(random.uniform(0,len(trainingSet))) testSet.append(trainingSet[randIndex]) del(trainingSet[randIndex]) trainMat = [] trainClasses = [] for docIndex in trainingSet: trainMat.append(setOfWords2Vec(vocabList, docList[docIndex])) trainClasses.append(classList[docIndex]) # 使用训练集得到概率向量 p0V, p1V, pSpam = trainNB0(array(trainMat), array(trainClasses)) # 测试分类器的错误率 errorCount = 0 for docIndex in testSet: wordVector = setOfWords2Vec(vocabList, docList[docIndex]) if classifyNB(array(wordVector), p0V, p1V, pSpam) != classList[docIndex]: errorCount += 1 print("Classification error:") print(docList[docIndex]) print(errorCount) print("the error rate is:", float(errorCount)/len(testSet)) github代码]]></content>
      <categories>
        <category>机器学习实战</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Logistic回归和Sigmoid函数]]></title>
    <url>%2F2017%2F11%2F03%2F%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2FwLogistic%E5%9B%9E%E5%BD%92%E5%92%8CSigmoid%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[一、概述 1. 原理： 工作机制： 2. 优缺点 优点：计算代价不高，易于理解和实现 缺点： 容易欠拟合，分类精度可能不高 适用数据范围：数值型和标称型数据 理论 ### 二、算法流程 1. 收集数据：anyway 2. 准备数据：需要数值型(要进行距离计算)，结构化数据格式最佳 3. 分析数据：anyway 4. 训练算法：大部分时间将用于训练，训练的目的是为了找到最佳的分类回归系数 5. 测试算法：一旦训练步骤完成，分类将会很快 6. 使用算法： * 步骤一：输入一些数据，并将其转换成对应的结构化数值； * 步骤二：基于训练好的回归系数对数值进行简单的回归计算，判定它们属于哪个类别；随后在输出的类别上做一些其他的分析工作 三、算法实践 错误合集 1.问题 准备数据 打开文件并逐行读取 123456789101112def loadDataSet(): # 定义数据集和标签 dataMat = [] labelMat = [] # 读取文件 fr = open('testSet.txt') for line in fr.readlines(): lineArr = line.strip().split() # 初始化数据 dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])]) labelMat.append(int(lineArr[2])) return dataMat, labelMat 123456&gt;&gt;&gt; import logRegres&gt;&gt;&gt; dataArr,labelMat = logRegres.loadDataSet()&gt;&gt;&gt; logRegres.gradAscent(dataArr,labelMat)matrix([[ 4.12414349], [ 0.48007329], [-0.6168482 ]]) 回归函数和梯度上升算法 1234567891011121314151617181920212223#回归函数def sigmoid(intX): return 1.0/(1+exp(-intX))# 梯度上升算法def gradAscent(dataMatIn,classLabels): # 转换为Numpy数据类型 dataMatrix = mat(dataMatIn) labelMat = mat(classLabels).transpose() # 矩阵大小 m, n = shape(dataMatrix) # 步长 alpha = 0.001 # 迭代次数 maxCycles = 500 # 系数矩阵初始化为1 weights = ones((n, 1)) for k in range(maxCycles): # 变量h是一个列向量，元素个数等于样本个数 h = sigmoid(dataMatrix*weights) error = (labelMat-h) weights = weights+alpha*dataMatrix.transpose()*error return weights 2.分析数据 利用Matplotlib画图 画出数据集和Logistic最佳拟合直线 12345678910111213141516171819202122232425262728293031323334353637# 画出最佳拟合直线def plotBestFit(wei): import matplotlib.pyplot as plt # 矩阵变为数组 weights = wei.getA() # 加载数据 dataMat, labelMat = loadDataSet() # 转化为数组 dataArr = array(dataMat) # 数据的列数目 n = shape(dataArr)[0] # 用于存放类1的点 xcord1 = [] ycord1 = [] # 用于存放类2的点 xcord2 = [] ycord2 = [] # 遍历所有点 for i in range(n): if(int(labelMat[i]) == 1): xcord1.append(dataArr[i, 1]) ycord1.append(dataArr[i, 2]) else: xcord2.append(dataArr[i, 1]) ycord2.append(dataArr[i, 2]) # 画出所有点的信息 fig = plt.figure() ax = fig.add_subplot(111) ax.scatter(xcord1, ycord1, s=30, c='red', marker='s') ax.scatter(xcord2, ycord2, s=30, c='green') x = arange(-3.0, 3.0, 0.1) # 画出分类的边界，函数的系数由之前的梯度上升算法求得 y = (-weights[0]-weights[1]*x)/weights[2] ax.plot(x, y) plt.xlabel('X1') plt.ylabel('X1') plt.show() 3.训练算法 缺点： 对于以上的算法，每次更新回归系数我们都需要遍历整个数据集，如果数据量过大，数亿或者成千上万个特征，那么==计算复杂度==就太高。 ##### 改进：每次仅用一个样本更改回归系数，这种方法就成为==随机梯度上升算法==。 这种在样本到来时对分类器进行增量更新的方式可以称为在线学习算法。相应的，一次处理所有数据称为批处理。 123456789101112# 随机梯度上升算法def stocGradAscent0(dataMatrix,classLabels): # 无矩阵转换过程 m, n = shape(dataMatrix) alpha = 0.01 weights = ones(n) for i in range(m): # 变量h和误差error都是数值 h = sigmoid(sum(dataMatrix[i]*weights)) error = (classLabels[i]-h) weights = weights + alpha * error * dataMatrix[i] return weights ##### 改进 * 从结果中可以看出分类效果并不是很好，这主要是和迭代的次数和步长有关系，我们进一步修改算法，让迭代步长随着迭代次数的增加逐渐变小。另外，我们可以随机选取样本更新系数。 12345678910111213141516def stocGradAscent1(dataMatrix, classLabels, numIter=150): m,n = shape(dataMatrix) weights = ones(n) #initialize to all ones for j in range(numIter): dataIndex = list(range(m)) for i in range(m): # alpha在每次迭代时不断减小，但不会减到0 alpha = 4/(1.0+j+i)+0.0001 # 随机选取更新 randIndex = int(random.uniform(0, len(dataIndex))) h = sigmoid(sum(dataMatrix[randIndex]*weights)) error = classLabels[randIndex] - h weights = weights + alpha * error * dataMatrix[randIndex] # 删除，进行下一次迭代 del(dataIndex[randIndex]) return weights 四.示例：预测病马的死亡率 1.问题：数据缺失 解决办法： 1.用可用特征的均值来替代 2.用特殊值来替代，如-1 3.忽略有缺失值的样本 4.使用相识样本的均值来替代 5.使用另外的机器学习算法预测缺失值 #### 2.准备数据：处理数据的缺失值 用实数0来替换所有的缺失值（==NumPy数据类型不允许包含缺失值==） 作者自己并没有给出具体的实现方法（待补充） 3.测试算法：用Logistic回归进行分类 1234567891011121314151617181920212223242526272829303132333435363738394041424344# 通过输入回归系数和特征向量来计算对应sigmoid的值def classifyVector(inX, weights): prob = sigmoid(sum(inX*weights)) if prob &gt; 0.5: return 1.0 else: return 0.0def colicTest(): # 导入数据 frTrain = open('horseColicTraining.txt') frTest = open('horseColicTest.txt') trainingSet = []; trainingLabels = [] for line in frTrain.readlines(): currLine = line.strip().split('\t') lineArr =[] for i in range(21): lineArr.append(float(currLine[i])) trainingSet.append(lineArr) trainingLabels.append(float(currLine[21])) # 导入数据完成后利用stocGradAscent1（）来计算回归系数向量 trainWeights = stocGradAscent1(array(trainingSet), trainingLabels, 1000) errorCount = 0 numTestVec = 0.0 # 导入测试集并计算分类错误率 for line in frTest.readlines(): numTestVec += 1.0 currLine = line.strip().split('\t') lineArr =[] for i in range(21): lineArr.append(float(currLine[i])) if int(classifyVector(array(lineArr), trainWeights))!= int(currLine[21]): errorCount += 1 errorRate = (float(errorCount)/numTestVec) print("the error rate of this test is: %f" % errorRate) return errorRate# 调用colicTest()函数10次并求结果的平均值def multiTest(): numTests = 10 errorSum=0.0 for k in range(numTests): errorSum += colicTest() print("after %d iterations the average error rate is: %f" % (numTests, errorSum/float(numTests))) 在有30%的数据缺失的情况下，得到平均错误率约为33% 通过调整colicTest()中的迭代次数和stocGradAscent1()中的步长，平均错误率可以降到20%左右]]></content>
      <categories>
        <category>机器学习实战</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实战二（决策树）]]></title>
    <url>%2F2017%2F11%2F02%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E4%BA%8C%EF%BC%88%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一. 决策树 1. 概念： 决策树学习是根据数据的属性采用树状结构建立的一种决策模型，可以用此模型解决分类和回归问题。常见的算法包括 CART(Classification And Regression Tree), ID3, C4.5等。 优点 易于理解和解释，甚至比线性回归更直观； 与人类做决策思考的思维习惯契合； 模型可以通过树的形式进行可视化展示； 可以直接处理非数值型数据，不需要进行哑变量的转化，甚至可以直接处理含缺失值的数据； 缺点： 对于有大量数值型输入和输出的问题，决策树未必是一个好的选择； 产生过拟合 特别是当数值型变量之间存在许多错综复杂的关系，如金融数据分析； 决定分类的因素取决于更多变量的复杂组合时； 模型不够稳健，某一个节点的小小变化可能导致整个树会有很大的不同。 二. 决策树算法 1. 概念：决策树算法主要是指决策树进行创建中进行树分裂(划分数据集)的时候选取最优特征的算法，他的主要目的就是要选取一个特征能够将分开的数据集尽量的规整，也就是尽可能的纯. 最大的原则就是: 将无序的数据变得更加有序 2. 决策树学习算法主要由三部分构成: 特征选择 决策树生成 决策树的剪枝 三. 特征选择 信息熵 熵(entropy)概念最早来源于统计热力学，它是热力学系统混乱程度的一种度量。系统的混乱程度越低，其熵值就越小。 定义9-2 设δ为可取n个离散数值的随机变量，它取δi的概率为p(δi) (i=1,2,…,n)，则我们定义 为随机变量的信息熵(Information Entropy)。 样本数据集S的任一属性A都可看作一个随机变量，假设其取值为{a1, a2 ,…, an}，则E(A)就是属性A所有取值的信息熵，其熵值越小所蕴含的不确定信息越小，越有利于数据的分类。 信息增益(information gain) 增益比率(gain ratio) 基尼不纯度(Gini impurity) 2. 代码实现 ID3算法为例 12345678910111213141516171819202122# 为所有可能的分类创建字典def uniquecounts(rows): results = &#123;&#125; for row in rows: # 计数结果在最后一列 r = row[len(row)-1] if r not in results:results[r] = 0 results[r]+=1 return results # 返回一个字典# 熵def entropy(rows): from math import log log2 = lambda x:log(x)/log(2) results = uniquecounts(rows) # 开始计算熵的值 ent = 0.0 for r in results.keys(): p = float(results[r])/len(rows) # 以2为底求对数 ent = ent - p*log2(p) return ent 四. 决策树的生成 1. 经典的实现算法： ID3算法 C4.5算法 CART算法 2. ID3的算法思想（依据信息增益进行特征选取和分裂） 从根节点开始，选择信息增益最大的特征作为结点的特征，并由该特征的不同取值构建子节点 对子节点递归地调用以上方法，构建决策树 直到所有特征的信息增益均很小或者没有特征可选时为止。 12345678910111213// 算法框架如下class DecisionTree(object): def fit(self, X, y): # 依据输入样本生成决策树 self.root = self._build_tree(X, y) def _build_tree(self, X, y, current_depth=0): #1. 选取最佳分割特征，生成左右节点 #2. 针对左右节点递归生成子树 def predict_value(self, x, tree=None): # 将输入样本传入决策树中，自顶向下进行判定 # 到达叶子节点即为预测值 3. C4.5算法 C4.5算法与ID3算法的区别主要在于它在生产决策树的过程中，使用信息增益比来进行特征选择。 4. CART算法 CART假设决策树是一个二叉树，它通过递归地二分每个特征，将特征空间划分为有限个单元，并在这些单元上确定预测的概率分布。 CART算法中，对于回归树，采用的是平方误差最小化准则；对于分类树，采用基尼指数最小化准则。 平方误差最小化 基尼指数 五. 决策树的剪枝 1. 过拟合问题的解决方法： 当熵减少的数量小于某一个阈值时，就停止分支的创建。这是一种贪心算法。（限制Gain的阈值） 先创建完整的决策树，然后再尝试消除多余的节点，也就是采用减枝的方法。 六. 完整代码 完整代码 github决策树代码实践]]></content>
      <categories>
        <category>机器学习实战</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一章-深度学习概论]]></title>
    <url>%2F2017%2F10%2F29%2F%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2Fw%E7%AC%AC%E4%B8%80%E7%AB%A0-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[1.2 什么是神经网络 ReLU函数（修正线性单元）：“修正”指的是取不小于0的值 图右是单个神经元：输入面积-&gt;computer运算-&gt;输出价格 多个神经元叠加构成一个更大的神经网络 左边的是输入的特征（输入层-input layer） 中间的圆圈在神经网络中被称为“隐藏单元”（Hidden nuit）：每个的输入都来着四个特征，因此是让神经网络自己决定中间的数代表的含义（why，这里为什么要这么说？） 1.3 用神经网络进行监督学习 神经网络架构 1.standard neural network architecture 通用标准的神经网络架构 convolutional neural network 卷积神经网络（CNN） 数据 Structured Data ：结构化数据（数据库），意为着每个特征都有清晰的定义，如数据库表中值的定义 Unstructured Data : 非结构化数据，图像、音频文本中的内容 ### 1.4为什么深度学习流行起来 数据量的增加 硬件设备的提升 算法的不断改进创新 规模推动深度学习的发展（神经网络性能随规模、数据的增长不断增长） x轴代表训练的规模 ，y轴代表表现 数据集不大时，效果取决于手工设计的组件 在图形左边时，各个算法的效率不是很明确 ex： 但在大数据方面，足够的M（训练集）的支持使神经网络的效率领先其他算法]]></content>
      <categories>
        <category>深度学习与神经网络(吴恩达)</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>机器学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习实战一（K-邻近算法（KNN））]]></title>
    <url>%2F2017%2F10%2F29%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E4%B8%80%EF%BC%88K-%E9%82%BB%E8%BF%91%E7%AE%97%E6%B3%95%EF%BC%88KNN%EF%BC%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一、概述 1. 原理： 工作机制：给定测试样本，基于某种距离度量找出训练集中的与其最靠近的k个训练样本，然后基于这k个“邻居”的信息来进行预测. 通常k是不大于20的整数 2. 优缺点 优点：精度高、对异常值不敏感、无数据输入假定 缺点：计算复杂度高、空间复杂度高，占用存储空间。无法给出数据的内在含义 适用数据范围：数值型和标称型 3. 懒惰算法： 此类学习技术在训练阶段仅仅把样本保存起来 训练开支为零，待收到测试样本后再进行处理 二、算法流程 收集数据 准备数据 分析数据 训练算法（不适合KNN算法，因为K是“懒惰算法”的著名代表） 测试算法 使用算法 三、算法框架 1. classify0()函数的4个输入参数： inX：用于分类的输入向量 dataSet：用于输入的训练样本集 labels：标签向量 k：参数k用于选择最佳邻居的数目 2. 使用欧氏距离公式 计算两个向量点xA和xB之间的距离 计算存在4个特征值的数据集的距离 3. 在约会网站上使用算法 海伦用三个属性来测试自己对于约会对象的喜爱程度 每年的飞行里程数 玩视频游戏所耗时间百分比 每周消费的冰激凌公升数 她积攒了一些数据，该数据保存在datingTestSet.txt中，共1000行。 准备数据 1234def createDataSet(): group = array([[1.0, 1.1], [1.0, 1.0], [0, 0], [0, 0.1]]) labels = ['A', 'A', 'B', 'B'] return group, labels 算法 1234567891011121314151617181920212223242526272829def classify0(inX,dataSet,labels,k): #训练数据集的行数 dataSetSize=dataSet.shape[0] #计算距离 #这里要说一下tile()函数，以后我们还会多次用到它 #tile(A,B)表示对A重复B次，B可以是int型也可以是数组形式 #如果B是int，表示在行方向上重复A，B次，列方向默认为1 #如果B是数组形式，tile(A,(B1,B2))表示在行方向上重复B1次，列方向重复B2次 diffMat=tile(inX,(dataSetSize,1))-dataSet print(diffMat) sqDiffMat=diffMat**2 print(sqDiffMat) sqDistances=sqDiffMat.sum(axis=1) distances=sqDistances**0.5 #排序，这里argsort()返回的是数据从小到大的索引值,这里这就是第几行数据 sortedDisIndicies =distances.argsort() print(sortedDisIndicies) classCount=&#123;&#125; #选取距离最小的k个点，并统计每个类别出现的频率 #这里用到了字典get(key,default=None)返回键值key对应的值； #如果key没有在字典里，则返回default参数的值，默认为None for i in range(k): voteIlabel=labels[sortedDisIndicies[i]] classCount[voteIlabel]=classCount.get(voteIlabel,0)+1; #逆序排序，找出出现频率最多的类别 sortedClassCount=sorted(classCount.items(), key=operator.itemgetter(1),reverse=True) print(sortedClassCount) return sortedClassCount[0][0] py3.5中iteritems改为items（上倒数第4行） 1234#进入python界面： &gt;&gt;&gt;import kNN &gt;&gt;&gt;group,labels=kNN.createDataSet() &gt;&gt;&gt;kNN.classify0([0,0],group,labels,3) 读取txt的数据 123456789101112131415161718192021def file2matrix(filename): fr=open(filename) #读取文件 arrayOLines=fr.readlines() #文件行数 numberOfLines=len(arrayOLines) #创建全0矩阵 returnMat=zeros((numberOfLines,3)) #标签向量 classLabelVector=[] index=0 #遍历每一行，提取数据 for line in arrayOLines: line=line.strip(); listFromLine=line.split('\t') #前三列为属性信息 returnMat[index,:]=listFromLine[0:3] #最后一列为标签信息 classLabelVector.append(int(listFromLine[-1])) index +=1 return returnMat,classLabelVector 123# 重新加载kNN，得倒解析数据 &gt;&gt;&gt;reload(kNN) &gt;&gt;&gt;datingDataMat,datingLabels=kNN.file2matrix('datingTestSet2.txt') 使用Matplotlib 创建散点图 12345678# 命令行中输入&gt;&gt;&gt;import matplotlib &gt;&gt;&gt;import matplotlib.pyplot as plt &gt;&gt;&gt;from numpy import array &gt;&gt;&gt;fig = plt.figure() &gt;&gt;&gt;ax=fig.add_subplot(111) &gt;&gt;&gt;ax.scatter(datingDataMat[:,1],datingDataMat[:,2],15.0*array(datingLabels),15.0*array(datingLabels))&gt;&gt;&gt;plt.show() 利用github存储图片并以markdown的链接形式显示 4. 数据的归一化 由于每个属性的取值数量级相差过大，会造成每个属性的权重不同，这显然是海伦不希望的。所以我们还要写一个函数实现数据归一化，公式如下: newValue=(oldValue-min)/(max-min) 这个公式可以将特征值转化为0~1之间的值。 1234567891011121314#归一化特征值def autoNorm(dataSet): #每列的最小值 minVals=dataSet.min(0) #每列的最大值 maxVals=dataSet.max(0) #最大值与最小值的差值 ranges=maxVals-minVals normDataSet=zeros(shape(dataSet)) m=dataSet.shape[0] #minVals是1*3的矩阵，使用tile函数复制成和dataSet同样大小的矩阵，方便计算 normDataSet=dataSet-tile(minVals,(m,1)) normDataSet=normDataSet/tile(ranges,(m,1)) return normDataSet,ranges,minVals 12&gt;&gt;&gt;reload(kNN) &gt;&gt;&gt;normMat,ranges,minVals=kNN.autoNorm(datingDataMat) 5. 原始分类器 1234567891011121314#原始测试分类器def datingClassTest(): hoRatio=0.10 datingDataMat,datingLabels=file2matrix('datingTestSet2.txt') normMat,ranges,minVals=autoNorm(datingDataMat) m=normMat.shape(0) #10%的数据用于测试数据集 numTestVecs=int(m*hoRatio) errorCount=0.0 for i in range(numTestVecs): classifierResults=classify0(normMat[i,:],normMat[numTestVecs:m,:], datingLabels[numTestVecs:m],3) print("the classifier came back with: %d,the real answer id: %d"%(classifierResults,datingLabels[i])) if(classifierResults!=datingLabels[i]):errorCount +=1.0 print("the total error rate is: %f" %(errorCount/float(numTestVecs))) 6. 接口 1234567891011# 约会网站预测函数def classifyPerson(): resultList=['not at all','in small doses','in large doses'] percentTats=float(input("在游戏上花费的时间占比( )?")) ffMiles=float(input("每年航空的里程数?")) iceCream=float(input("每年吃的冰淇淋（升）?")) datingDataMat,datingLabels=file2matrix('datingTestSet2.txt') normMat, ranges, minVals=autoNorm(datingDataMat) inArr=array([ffMiles,percentTats,iceCream]) classifierResult=classify0((inArr-minVals)/ranges,normMat,datingLabels,3) print("你可能是属于以下这类人：",resultList[classifierResult - 1]) 7. 识别手写数字 1234567891011# 将图片格式处理为一个向量（把一个32*32的二进制图形矩阵转换为1*1024的向量）,然后使用分类器处理图形信息def img2vector(filename): # 创建一个1*1024的NumPy数组 returnVect = zeros((1,1024)) fr = open(filename) # 循环读出文件的前32行，并将每行的头32个字符值存储在NumPy数组中，最后返回数组 for i in range(32): lineStr = fr.readline() for j in range(32): returnVect[0,32*i+j] = int(lineStr[j]) return returnVect 123456789101112131415161718192021222324252627282930313233def handwritingClassTest(): hwLabels = [] # 将trainingDigits目录中的文件存储在列表中 trainingFileList = listdir('trainingDigits') # 获得文件数赋值给m m = len(trainingFileList) # 创建一个m行1024列的训练矩阵，每行数据存储一个图像 trainingMat = zeros((m,1024)) for i in range(m): # 从文件名解析分类数字（eg：文件9_45.txt表示数字9的第45个实例） fileNameStr = trainingFileList[i] fileStr = fileNameStr.split('.')[0] #take off .txt classNumStr = int(fileStr.split('_')[0]) # 将类代码存储在 hwLabels向量中 hwLabels.append(classNumStr) # 使用 img2vector函数载入图像 trainingMat[i,:] = img2vector('trainingDigits/%s' % fileNameStr) # 将testDigits目录中的文件存储在列表中，步骤同上 testFileList = listdir('testDigits') #iterate through the test set errorCount = 0.0 mTest = len(testFileList) for i in range(mTest): fileNameStr = testFileList[i] fileStr = fileNameStr.split('.')[0] #take off .txt classNumStr = int(fileStr.split('_')[0]) vectorUnderTest = img2vector('testDigits/%s' % fileNameStr) # 不同的是要使用 classify0函数测试目录下的每个文件（文件中的值介于0-1之间，无需用autoNorm()函数 classifierResult = classify0(vectorUnderTest, trainingMat, hwLabels, 3) print("the classifier came back with: %d, the real answer is: %d" % (classifierResult, classNumStr)) if (classifierResult != classNumStr): errorCount += 1.0 print("\nthe total number of errors is: %d" % errorCount) print("\nthe total error rate is: %f" % (errorCount/float(mTest))) github代码]]></content>
      <categories>
        <category>机器学习实战</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘-关联规则挖掘]]></title>
    <url>%2F2017%2F01%2F02%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%8C%96%E6%8E%98%2F</url>
    <content type="text"><![CDATA[概念 基本概念 mark k-项集与MinS 关联规则、支持度、频繁 k- 项集 支持度：T中同时包含X和Y的事务在R中所占的百分比 置信度 mark 强关联规则 mark Apriori算法 mark 频繁相集发现算法 #### 产生关联规则 #### 关联规则性质 FP-增长算法 mark]]></content>
      <categories>
        <category>数据挖掘</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>数据挖掘</tag>
      </tags>
  </entry>
</search>
